{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2g87sh7lizq",
   "source": "# Sklearn Built-in Tabular Classification Datasets — Demo\n\nThis notebook demonstrates the data collection pipeline for **sklearn built-in tabular classification datasets**. The original script loads the Breast Cancer Wisconsin (569 samples, 30 features, binary classification) and Wine (178 samples, 13 features, 3-class) datasets from `sklearn.datasets`, then:\n\n1. **Standardizes** continuous features with `StandardScaler`\n2. **Discretizes** features using `KBinsDiscretizer` (5-bin and 10-bin quantile)\n3. **Generates** 5-fold `StratifiedKFold` cross-validation indices\n4. **Outputs** each row as a separate example with JSON feature dicts and class labels\n\n**Part 1** runs a quick demo on a small curated subset.\n**Part 2** runs the full pipeline with all original parameters.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lg36el1t47",
   "source": "import json\n\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer, load_wine\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import KBinsDiscretizer, StandardScaler\n\nimport matplotlib.pyplot as plt",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "mmgu3wq7qni",
   "source": "GITHUB_FULL_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-ac2586-synergy-guided-oblique-splits-using-part/main/sklearn_tabular/demo/full_demo_data.json\"\nGITHUB_MINI_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-ac2586-synergy-guided-oblique-splits-using-part/main/sklearn_tabular/demo/mini_demo_data.json\"\nimport json, os\n\ndef _load_json(url, local_path):\n    try:\n        import urllib.request\n        with urllib.request.urlopen(url) as response:\n            return json.loads(response.read().decode())\n    except Exception: pass\n    if os.path.exists(local_path):\n        with open(local_path) as f: return json.load(f)\n    raise FileNotFoundError(f\"Could not load {local_path}\")\n\ndef load_mini():\n    return _load_json(GITHUB_MINI_DATA_URL, \"mini_demo_data.json\")\n\ndef load_full():\n    return _load_json(GITHUB_FULL_DATA_URL, \"full_demo_data.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7ebpqcps59",
   "source": "## Part 1 — Quick Demo (Mini Data)\n\nLoad a small curated subset (11 examples across 2 datasets) and run the processing pipeline with reduced parameters for a fast demo.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5cb0fsr2ffh",
   "source": "data = load_mini()\n\nprint(f\"Number of datasets: {len(data['datasets'])}\")\nfor ds in data[\"datasets\"]:\n    print(f\"  {ds['dataset']}: {len(ds['examples'])} examples\")\nprint(f\"\\nFirst example keys: {list(data['datasets'][0]['examples'][0].keys())}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4xggemunksm",
   "source": "### Process Dataset\n\nThe core `process_dataset` function from the original script loads a sklearn dataset, applies StandardScaler, KBinsDiscretizer (5 and 10 bins), and assigns StratifiedKFold indices. Each row becomes a separate example with JSON feature dict as input and class label as output.\n\nFor the quick demo, we use **n_folds=2** (instead of 5) to speed things up.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ab53bdi6vkr",
   "source": "def process_dataset(\n    name: str,\n    loader_func,\n    n_folds: int = 5,\n    random_state: int = 42,\n) -> dict:\n    \"\"\"Process a single sklearn dataset into the standardized schema.\"\"\"\n    data = loader_func()\n    X_raw = data.data\n    y = data.target\n    feature_names = [str(f) for f in data.feature_names]\n    target_names = [str(t) for t in data.target_names]\n    n_samples, n_features = X_raw.shape\n    n_classes = len(target_names)\n\n    print(f\"  Processing {name}: {n_samples} samples, {n_features} features, {n_classes} classes\")\n\n    # 1. Standardize continuous features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_raw)\n\n    # 2. Discretize with KBinsDiscretizer (5 bins and 10 bins)\n    discretizer_5 = KBinsDiscretizer(\n        n_bins=5,\n        encode=\"ordinal\",\n        strategy=\"quantile\",\n        subsample=None,\n    )\n    X_disc_5 = discretizer_5.fit_transform(X_raw)\n\n    discretizer_10 = KBinsDiscretizer(\n        n_bins=10,\n        encode=\"ordinal\",\n        strategy=\"quantile\",\n        subsample=None,\n    )\n    X_disc_10 = discretizer_10.fit_transform(X_raw)\n\n    # 3. Generate StratifiedKFold indices\n    skf = StratifiedKFold(\n        n_splits=n_folds,\n        shuffle=True,\n        random_state=random_state,\n    )\n    fold_assignments = np.zeros(n_samples, dtype=int)\n    for fold_idx, (_, test_idx) in enumerate(skf.split(X_raw, y)):\n        fold_assignments[test_idx] = fold_idx\n\n    # 4. Build examples (one per row)\n    examples = []\n    for i in range(n_samples):\n        # Build input as JSON string of feature values (using original continuous values)\n        feature_dict = {}\n        for j, fname in enumerate(feature_names):\n            feature_dict[fname] = round(float(X_raw[i, j]), 6)\n\n        # Build the input string: JSON representation of feature values\n        input_str = json.dumps(feature_dict)\n\n        # Output: target class name as string\n        output_str = str(target_names[y[i]])\n\n        example = {\n            \"input\": input_str,\n            \"output\": output_str,\n            \"metadata_fold\": int(fold_assignments[i]),\n            \"metadata_feature_names\": feature_names,\n            \"metadata_task_type\": \"classification\",\n            \"metadata_n_classes\": n_classes,\n            \"metadata_row_index\": i,\n            \"metadata_n_features\": n_features,\n            \"metadata_n_samples\": n_samples,\n            \"metadata_target_names\": target_names,\n            \"metadata_standardized_values\": [round(float(v), 6) for v in X_scaled[i]],\n            \"metadata_discretized_5bin\": [int(v) for v in X_disc_5[i]],\n            \"metadata_discretized_10bin\": [int(v) for v in X_disc_10[i]],\n        }\n        examples.append(example)\n\n    print(f\"  -> Generated {len(examples)} examples for {name}\")\n    return {\"dataset\": name, \"examples\": examples}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fxydtv7p1dr",
   "source": "### Run Processing (Quick Demo)\n\nRun the pipeline on both datasets with **n_folds=2** for a quick demo. The original script uses `n_folds=5`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "rdhzyf6gar",
   "source": "print(\"=\" * 60)\nprint(\"Loading sklearn built-in tabular classification datasets\")\nprint(\"=\" * 60)\n\ndatasets_config = [\n    (\"breast_cancer_wisconsin\", load_breast_cancer),\n    (\"wine\", load_wine),\n]\n\n# Quick demo: n_folds=2 (original: n_folds=5)\nall_datasets = []\nfor name, loader in datasets_config:\n    dataset_entry = process_dataset(name=name, loader_func=loader, n_folds=2)\n    all_datasets.append(dataset_entry)\n\noutput = {\"datasets\": all_datasets}\n\n# Summary\ntotal_examples = sum(len(d[\"examples\"]) for d in all_datasets)\nprint(f\"\\nTotal datasets: {len(all_datasets)}\")\nprint(f\"Total examples: {total_examples}\")\nfor d in all_datasets:\n    print(f\"  {d['dataset']}: {len(d['examples'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9ibvtg0tog9",
   "source": "### Inspect Pre-generated Data\n\nCompare the freshly processed output with the pre-generated mini dataset loaded from JSON.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2qm87ezu6ws",
   "source": "# Inspect a pre-generated example from the mini dataset\nfor ds in data[\"datasets\"]:\n    ex = ds[\"examples\"][0]\n    print(f\"--- {ds['dataset']} (first example) ---\")\n    print(f\"  Output label: {ex['output']}\")\n    print(f\"  Fold: {ex['metadata_fold']}\")\n    print(f\"  Task type: {ex['metadata_task_type']}\")\n    print(f\"  N classes: {ex['metadata_n_classes']}\")\n    print(f\"  Target names: {ex['metadata_target_names']}\")\n    print(f\"  N features: {ex['metadata_n_features']}\")\n    features = json.loads(ex[\"input\"])\n    print(f\"  Input features (first 5): {dict(list(features.items())[:5])}\")\n    print(f\"  Standardized (first 5): {ex['metadata_standardized_values'][:5]}\")\n    print(f\"  Discretized 5-bin (first 5): {ex['metadata_discretized_5bin'][:5]}\")\n    print(f\"  Discretized 10-bin (first 5): {ex['metadata_discretized_10bin'][:5]}\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1tlfd6vjq36h",
   "source": "### Visualize Results\n\nReusable visualization: class distribution, feature value distributions (standardized), and fold assignment counts for each dataset.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "fne0rmegi7g",
   "source": "def visualize_results(datasets_list, title_prefix=\"\"):\n    \"\"\"Reusable visualization for processed dataset results.\"\"\"\n    n_datasets = len(datasets_list)\n    fig, axes = plt.subplots(n_datasets, 3, figsize=(15, 4 * n_datasets))\n    if n_datasets == 1:\n        axes = axes[np.newaxis, :]\n\n    for idx, ds in enumerate(datasets_list):\n        name = ds[\"dataset\"]\n        examples = ds[\"examples\"]\n\n        # 1. Class distribution\n        labels = [ex[\"output\"] for ex in examples]\n        unique_labels, counts = np.unique(labels, return_counts=True)\n        axes[idx, 0].bar(unique_labels, counts, color=plt.cm.Set2.colors[:len(unique_labels)])\n        axes[idx, 0].set_title(f\"{name}\\nClass Distribution\")\n        axes[idx, 0].set_ylabel(\"Count\")\n        axes[idx, 0].tick_params(axis=\"x\", rotation=15)\n\n        # 2. Standardized feature distributions (first 5 features)\n        std_vals = np.array([ex[\"metadata_standardized_values\"] for ex in examples])\n        feature_names = examples[0][\"metadata_feature_names\"]\n        n_show = min(5, std_vals.shape[1])\n        bp = axes[idx, 1].boxplot(\n            [std_vals[:, j] for j in range(n_show)],\n            labels=[fn[:12] for fn in feature_names[:n_show]],\n            patch_artist=True,\n        )\n        for patch, color in zip(bp[\"boxes\"], plt.cm.Set2.colors):\n            patch.set_facecolor(color)\n        axes[idx, 1].set_title(f\"{name}\\nStandardized Features (first {n_show})\")\n        axes[idx, 1].tick_params(axis=\"x\", rotation=30)\n\n        # 3. Fold distribution\n        folds = [ex[\"metadata_fold\"] for ex in examples]\n        unique_folds, fold_counts = np.unique(folds, return_counts=True)\n        axes[idx, 2].bar(unique_folds.astype(str), fold_counts, color=\"steelblue\")\n        axes[idx, 2].set_title(f\"{name}\\nFold Distribution\")\n        axes[idx, 2].set_xlabel(\"Fold\")\n        axes[idx, 2].set_ylabel(\"Count\")\n\n    fig.suptitle(f\"{title_prefix}Dataset Processing Results\", fontsize=14, fontweight=\"bold\", y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n    # Print summary table\n    print(f\"\\n{'Dataset':<30} {'Examples':>8} {'Features':>8} {'Classes':>8} {'Folds':>6}\")\n    print(\"-\" * 62)\n    for ds in datasets_list:\n        ex0 = ds[\"examples\"][0]\n        n_folds = len(set(ex[\"metadata_fold\"] for ex in ds[\"examples\"]))\n        print(f\"{ds['dataset']:<30} {len(ds['examples']):>8} {ex0['metadata_n_features']:>8} {ex0['metadata_n_classes']:>8} {n_folds:>6}\")\n\n# Visualize quick demo results\nvisualize_results(all_datasets, title_prefix=\"Quick Demo: \")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vkvmn8lzi9",
   "source": "## Part 2 — Full Run (Original Parameters)\n\nLoad the complete dataset (747 examples across 2 datasets) and re-run the processing pipeline with **all original parameters** restored: `n_folds=5` (5-fold StratifiedKFold), `random_state=42`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hl5qql7v89a",
   "source": "data = load_full()\n\nprint(f\"Full dataset: {len(data['datasets'])} datasets\")\nfor ds in data[\"datasets\"]:\n    print(f\"  {ds['dataset']}: {len(ds['examples'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "x6ikv07fd3h",
   "source": "print(\"=\" * 60)\nprint(\"Full Run — Original Parameters (n_folds=5)\")\nprint(\"=\" * 60)\n\ndatasets_config = [\n    (\"breast_cancer_wisconsin\", load_breast_cancer),\n    (\"wine\", load_wine),\n]\n\n# Full run: original n_folds=5\nall_datasets_full = []\nfor name, loader in datasets_config:\n    dataset_entry = process_dataset(name=name, loader_func=loader, n_folds=5)\n    all_datasets_full.append(dataset_entry)\n\noutput_full = {\"datasets\": all_datasets_full}\n\n# Summary\ntotal_examples = sum(len(d[\"examples\"]) for d in all_datasets_full)\nprint(f\"\\nTotal datasets: {len(all_datasets_full)}\")\nprint(f\"Total examples: {total_examples}\")\nfor d in all_datasets_full:\n    print(f\"  {d['dataset']}: {len(d['examples'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "by579m9u3ng",
   "source": "# Visualize full run results\nvisualize_results(all_datasets_full, title_prefix=\"Full Run: \")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}