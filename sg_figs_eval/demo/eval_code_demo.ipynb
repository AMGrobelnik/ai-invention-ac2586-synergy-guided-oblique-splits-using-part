{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gsipf71cczh",
   "source": "# SG-FIGS Comprehensive Statistical Evaluation — Demo\n\nThis notebook demonstrates the **seven-block statistical evaluation** of SG-FIGS experiment results across 12 datasets and 6 methods.\n\n**Evaluation Blocks:**\n1. Per-dataset accuracy/AUC tables with Bonferroni-corrected paired t-tests (15 pairs)\n2. Critical difference diagram data (Friedman test + Nemenyi CD with clique identification)\n3. Spearman correlations between SG-FIGS-25 advantage and dataset properties\n4. Oblique split activation analysis with degeneration detection\n5. Accuracy-at-matched-complexity curves across max_rules={5,10,15}\n6. Ablation decomposition (oblique penalty vs synergy effect)\n7. Positive case study narratives\n\n**Part 1** runs a quick demo on a curated subset (3 datasets).  \n**Part 2** runs the full analysis on all 12 datasets with original parameters.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "762jcd96e5u",
   "source": "\"\"\"Imports — copied from eval.py with minimal additions for notebook context.\"\"\"\n\nimport json\nimport math\nimport sys\nfrom collections import defaultdict\nfrom itertools import combinations\n\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams.update({\"font.size\": 11})",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8q9hb76hrd6",
   "source": "\"\"\"Data loading helpers — GitHub URL with local fallback for Colab compatibility.\"\"\"\n\nGITHUB_FULL_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-ac2586-synergy-guided-oblique-splits-using-part/main/sg_figs_eval/demo/full_demo_data.json\"\nGITHUB_MINI_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-ac2586-synergy-guided-oblique-splits-using-part/main/sg_figs_eval/demo/mini_demo_data.json\"\nimport json, os\n\ndef _load_json(url, local_path):\n    try:\n        import urllib.request\n        with urllib.request.urlopen(url) as response:\n            return json.loads(response.read().decode())\n    except Exception: pass\n    if os.path.exists(local_path):\n        with open(local_path) as f: return json.load(f)\n    raise FileNotFoundError(f\"Could not load {local_path}\")\n\ndef load_mini():\n    return _load_json(GITHUB_MINI_DATA_URL, \"mini_demo_data.json\")\n\ndef load_full():\n    return _load_json(GITHUB_FULL_DATA_URL, \"full_demo_data.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "n3h184qkmb",
   "source": "---\n## Part 1 — Quick Demo (Mini Data: 3 Datasets)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dbossx880e",
   "source": "data = load_mini()\nprint(f\"Loaded mini data: {len(data['datasets'])} datasets, \"\n      f\"{sum(len(d['examples']) for d in data['datasets'])} examples\")\nprint(f\"Datasets: {[d['dataset'] for d in data['datasets']]}\")\nprint(f\"Aggregate metrics: {len(data['metrics_agg'])} keys\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "pwyqbm7lnfl",
   "source": "### Constants and Helper Functions\n\nEvaluation constants and statistical helper functions copied from the original script.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "444m3yhmdhr",
   "source": "# ---------------------------------------------------------------------------\n# Constants (from eval.py)\n# ---------------------------------------------------------------------------\nMETHODS = [\"FIGS\", \"RO-FIGS\", \"SG-FIGS-10\", \"SG-FIGS-25\", \"SG-FIGS-50\", \"GradientBoosting\"]\nMAX_RULES_VALUES = [5, 10, 15]\nN_FOLDS = 5\nALPHA = 0.05\nN_METHODS = len(METHODS)\nN_PAIRS = N_METHODS * (N_METHODS - 1) // 2  # 15\nBONFERRONI_ALPHA = ALPHA / N_PAIRS\n\n\n# ---------------------------------------------------------------------------\n# Helpers (from eval.py)\n# ---------------------------------------------------------------------------\n\ndef _safe_float(val):\n    \"\"\"Convert to float, handling NaN / None.\"\"\"\n    if val is None:\n        return float(\"nan\")\n    f = float(val)\n    return f\n\n\ndef _bonferroni_sig(p: float) -> str:\n    \"\"\"Return significance indicator under Bonferroni correction.\"\"\"\n    if p < 0.001 / N_PAIRS:\n        return \"***\"\n    if p < 0.01 / N_PAIRS:\n        return \"**\"\n    if p < BONFERRONI_ALPHA:\n        return \"*\"\n    return \"ns\"\n\n\ndef _paired_ttest(a: list[float], b: list[float]) -> tuple[float, float]:\n    \"\"\"Two-sided paired t-test; returns (t_stat, p_value).\n\n    Falls back to (0.0, 1.0) when variance is zero.\n    \"\"\"\n    a_arr = np.array(a, dtype=np.float64)\n    b_arr = np.array(b, dtype=np.float64)\n    diff = a_arr - b_arr\n    if np.std(diff, ddof=1) == 0:\n        return 0.0, 1.0\n    t_stat, p_val = stats.ttest_rel(a_arr, b_arr)\n    return float(t_stat), float(p_val)\n\n\ndef _rank_methods_per_dataset(\n    method_means: dict[str, float],\n) -> dict[str, float]:\n    \"\"\"Rank methods (1 = best) by accuracy; ties get average rank.\"\"\"\n    sorted_methods = sorted(method_means.items(), key=lambda x: -x[1])\n    ranks: dict[str, float] = {}\n    i = 0\n    while i < len(sorted_methods):\n        j = i + 1\n        while j < len(sorted_methods) and np.isclose(sorted_methods[j][1], sorted_methods[i][1]):\n            j += 1\n        avg_rank = np.mean(list(range(i + 1, j + 1)))\n        for k in range(i, j):\n            ranks[sorted_methods[k][0]] = float(avg_rank)\n        i = j\n    return ranks\n\n\ndef _nemenyi_q_alpha(k: int, alpha: float = 0.05) -> float:\n    \"\"\"Critical value for Nemenyi test (Studentized range / sqrt(2)).\n\n    Hardcoded for k=6, alpha=0.05 (from standard tables).\n    \"\"\"\n    if k == 6 and alpha == 0.05:\n        return 4.030 / math.sqrt(2)  # approx 2.850\n    raise ValueError(f\"Nemenyi q not hardcoded for k={k}, alpha={alpha}\")\n\n\nprint(\"Constants and helpers loaded.\")\nprint(f\"Methods: {METHODS}\")\nprint(f\"N_PAIRS (Bonferroni): {N_PAIRS}, Bonferroni alpha: {BONFERRONI_ALPHA:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xe5q79oubfp",
   "source": "### Reconstruct Structured Data\n\nParse the eval output into the nested structure `{dataset: {method: {max_rules: {fold: {metric: val}}}}}` that the analysis blocks expect. This adapts the original `load_iter4_data()` to work with the eval output format.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "a7uz0ymtzmk",
   "source": "def restructure_eval_data(raw_data: dict) -> dict:\n    \"\"\"Reconstruct the nested structure from eval output examples.\n\n    Returns: {dataset: {method: {max_rules: {fold: {accuracy, auc, ...}}}}}\n    \"\"\"\n    structured = {}\n    for ds_block in raw_data[\"datasets\"]:\n        ds_name = ds_block[\"dataset\"]\n        structured[ds_name] = {}\n        for ex in ds_block[\"examples\"]:\n            # Parse input to get method, fold, max_rules\n            inp = json.loads(ex[\"input\"])\n            method = inp[\"method\"]\n            fold = inp[\"fold\"]\n            max_rules = inp[\"max_rules\"]\n            # Parse output for per-fold metrics\n            out = json.loads(ex[\"output\"])\n            structured.setdefault(ds_name, {}).setdefault(method, {}).setdefault(max_rules, {})[fold] = {\n                \"accuracy\": _safe_float(out.get(\"accuracy\", 0)),\n                \"auc\": _safe_float(out.get(\"auc\", 0)),\n                \"oblique_fraction\": _safe_float(out.get(\"oblique_fraction\", 0)),\n                \"n_oblique\": out.get(\"n_oblique\", 0),\n                # Eval-level metrics from the example\n                \"eval_oblique_fraction\": ex.get(\"eval_oblique_fraction\", 0.0),\n                \"eval_degeneration_flag\": ex.get(\"eval_degeneration_flag\", 0.0),\n            }\n    return structured\n\n# Restructure loaded data\nstructured_data = restructure_eval_data(data)\ndatasets = list(structured_data.keys())\nprint(f\"Restructured data: {len(datasets)} datasets\")\nfor ds in datasets:\n    methods = list(structured_data[ds].keys())\n    print(f\"  {ds}: {len(methods)} methods\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4t9r8e542bf",
   "source": "### Block 1: Per-Dataset Accuracy/AUC Tables\n\nComputes mean±std accuracy and AUC per method per dataset, with Bonferroni-corrected paired t-tests across all 15 method pairs.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7blumtfs3sc",
   "source": "# ---------------------------------------------------------------------------\n# Block 1: Per-Dataset Accuracy/AUC Tables (from eval.py)\n# ---------------------------------------------------------------------------\n\ndef block1_per_dataset_accuracy(\n    data: dict,\n    datasets: list[str],\n    max_rules: int = 10,\n) -> dict:\n    \"\"\"For each dataset at max_rules=10, compute mean+/-std accuracy and AUC,\n    and run Bonferroni-corrected paired t-tests across all 15 method pairs.\"\"\"\n    results = {}\n    for ds in datasets:\n        ds_data = data.get(ds, {})\n        method_accs: dict[str, list[float]] = {}\n        method_aucs: dict[str, list[float]] = {}\n        for method in METHODS:\n            accs = []\n            aucs = []\n            mr_data = ds_data.get(method, {}).get(max_rules, {})\n            for fold in range(N_FOLDS):\n                fold_data = mr_data.get(fold, {})\n                accs.append(fold_data.get(\"accuracy\", float(\"nan\")))\n                aucs.append(fold_data.get(\"auc\", float(\"nan\")))\n            method_accs[method] = accs\n            method_aucs[method] = aucs\n\n        # Method summaries\n        method_stats = {}\n        for method in METHODS:\n            accs = np.array(method_accs[method])\n            aucs = np.array(method_aucs[method])\n            method_stats[method] = {\n                \"accuracy_mean\": float(np.nanmean(accs)),\n                \"accuracy_std\": float(np.nanstd(accs, ddof=1)),\n                \"auc_mean\": float(np.nanmean(aucs)),\n                \"auc_std\": float(np.nanstd(aucs, ddof=1)),\n            }\n\n        # Pairwise t-tests (Bonferroni)\n        pairwise = {}\n        method_pairs = list(combinations(METHODS, 2))\n        for m1, m2 in method_pairs:\n            t_stat, p_val = _paired_ttest(method_accs[m1], method_accs[m2])\n            p_corrected = min(p_val * N_PAIRS, 1.0)\n            sig = _bonferroni_sig(p_val)\n            pairwise[f\"{m1}_vs_{m2}\"] = {\n                \"t_statistic\": t_stat,\n                \"p_value_raw\": p_val,\n                \"p_value_corrected\": p_corrected,\n                \"significant\": sig,\n                \"mean_diff\": method_stats[m1][\"accuracy_mean\"] - method_stats[m2][\"accuracy_mean\"],\n            }\n\n        # Rank methods for this dataset\n        means_dict = {m: method_stats[m][\"accuracy_mean\"] for m in METHODS}\n        ranks = _rank_methods_per_dataset(means_dict)\n\n        # Delta vs FIGS\n        figs_mean = method_stats[\"FIGS\"][\"accuracy_mean\"]\n\n        results[ds] = {\n            \"method_stats\": method_stats,\n            \"pairwise_tests\": pairwise,\n            \"ranks\": ranks,\n            \"delta_vs_figs\": {m: method_stats[m][\"accuracy_mean\"] - figs_mean for m in METHODS},\n        }\n\n    return results\n\nblock1 = block1_per_dataset_accuracy(data=structured_data, datasets=datasets, max_rules=10)\n\n# Print summary table\nprint(\"Block 1: Per-Dataset Accuracy (max_rules=10)\")\nprint(f\"{'Dataset':<15}\", end=\"\")\nfor m in METHODS:\n    print(f\"{m:>16}\", end=\"\")\nprint()\nprint(\"-\" * (15 + 16 * len(METHODS)))\nfor ds in datasets:\n    print(f\"{ds:<15}\", end=\"\")\n    for m in METHODS:\n        s = block1[ds][\"method_stats\"][m]\n        print(f\"  {s['accuracy_mean']:.3f}±{s['accuracy_std']:.3f}\", end=\"\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fkq6l3mrcm8",
   "source": "### Block 2: Critical Difference Diagram Data\n\nFriedman test for overall method differences, plus Nemenyi CD for pairwise comparisons and clique identification.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "t0n6h20jqr",
   "source": "# ---------------------------------------------------------------------------\n# Block 2: Critical Difference Diagram Data (from eval.py)\n# ---------------------------------------------------------------------------\n\ndef block2_critical_difference(\n    data: dict,\n    datasets: list[str],\n    max_rules: int = 10,\n) -> dict:\n    \"\"\"Friedman test + Nemenyi CD computation.\"\"\"\n    n_datasets = len(datasets)\n    rank_matrix = np.zeros((n_datasets, N_METHODS))\n    mean_acc_matrix = np.zeros((n_datasets, N_METHODS))\n\n    for i, ds in enumerate(datasets):\n        ds_data = data.get(ds, {})\n        means = {}\n        for j, method in enumerate(METHODS):\n            mr_data = ds_data.get(method, {}).get(max_rules, {})\n            accs = [mr_data.get(fold, {}).get(\"accuracy\", float(\"nan\")) for fold in range(N_FOLDS)]\n            mean_acc = float(np.nanmean(accs))\n            means[method] = mean_acc\n            mean_acc_matrix[i, j] = mean_acc\n\n        ranks = _rank_methods_per_dataset(means)\n        for j, method in enumerate(METHODS):\n            rank_matrix[i, j] = ranks[method]\n\n    # Friedman test\n    chi2, p_friedman = stats.friedmanchisquare(*[rank_matrix[:, j] for j in range(N_METHODS)])\n\n    # Mean ranks\n    mean_ranks = {METHODS[j]: float(np.mean(rank_matrix[:, j])) for j in range(N_METHODS)}\n\n    # Nemenyi CD\n    k = N_METHODS\n    n = n_datasets\n    q_alpha = _nemenyi_q_alpha(k=k, alpha=0.05)\n    cd = q_alpha * math.sqrt(k * (k + 1) / (6 * n))\n\n    # Identify cliques\n    sorted_methods = sorted(mean_ranks.items(), key=lambda x: x[1])\n    cliques = []\n    for i in range(len(sorted_methods)):\n        clique = [sorted_methods[i][0]]\n        for j in range(i + 1, len(sorted_methods)):\n            if sorted_methods[j][1] - sorted_methods[i][1] < cd:\n                clique.append(sorted_methods[j][0])\n        if len(clique) > 1:\n            is_subset = any(set(clique).issubset(set(c)) for c in cliques)\n            if not is_subset:\n                cliques.append(clique)\n\n    result = {\n        \"friedman_chi_sq\": float(chi2),\n        \"friedman_p\": float(p_friedman),\n        \"mean_ranks\": mean_ranks,\n        \"nemenyi_cd\": cd,\n        \"nemenyi_q_alpha\": q_alpha,\n        \"cd_cliques\": cliques,\n        \"sorted_methods\": [m for m, _ in sorted_methods],\n        \"sorted_ranks\": [float(r) for _, r in sorted_methods],\n    }\n    return result\n\nblock2 = block2_critical_difference(data=structured_data, datasets=datasets, max_rules=10)\n\nprint(f\"Friedman chi² = {block2['friedman_chi_sq']:.2f}, p = {block2['friedman_p']:.2e}\")\nprint(f\"Nemenyi CD = {block2['nemenyi_cd']:.3f}\")\nprint(\"\\nMean Ranks:\")\nfor m, r in sorted(block2[\"mean_ranks\"].items(), key=lambda x: x[1]):\n    print(f\"  {m:<20} {r:.2f}\")\nprint(f\"\\nCD Cliques: {block2['cd_cliques']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "61vo3mgggzw",
   "source": "### Block 5: Accuracy-at-Matched-Complexity Curves\n\nComputes mean accuracy at `max_rules={5,10,15}` for each method and tests whether SG-FIGS-25 peaks at lower complexity than FIGS.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tmgfl1byxzr",
   "source": "# ---------------------------------------------------------------------------\n# Block 5: Accuracy-at-Matched-Complexity Curves (from eval.py)\n# ---------------------------------------------------------------------------\n\ndef block5_complexity_curves(\n    data: dict,\n    datasets: list[str],\n) -> dict:\n    \"\"\"Compute mean accuracy at max_rules={5,10,15} for each method.\"\"\"\n    results = {}\n    for method in METHODS:\n        mr_results = {}\n        for mr in MAX_RULES_VALUES:\n            all_accs = []\n            for ds in datasets:\n                ds_data = data.get(ds, {})\n                mr_data = ds_data.get(method, {}).get(mr, {})\n                for fold in range(N_FOLDS):\n                    acc = mr_data.get(fold, {}).get(\"accuracy\", float(\"nan\"))\n                    all_accs.append(acc)\n            mean_acc = float(np.nanmean(all_accs))\n            mr_results[mr] = mean_acc\n        # Peak\n        peak_mr = max(mr_results, key=mr_results.get)\n        peak_acc = mr_results[peak_mr]\n        efficiency_ratio = peak_acc / peak_mr if peak_mr > 0 else 0.0\n        results[method] = {\n            \"accuracy_at_mr5\": mr_results[5],\n            \"accuracy_at_mr10\": mr_results[10],\n            \"accuracy_at_mr15\": mr_results[15],\n            \"peak_max_rules\": peak_mr,\n            \"peak_accuracy\": peak_acc,\n            \"efficiency_ratio\": efficiency_ratio,\n        }\n\n    # Per-dataset peak complexity sign test\n    sg25_lower_count = 0\n    figs_lower_count = 0\n    ties = 0\n    for ds in datasets:\n        ds_data = data.get(ds, {})\n        for target_method, counter_method in [(\"SG-FIGS-25\", \"FIGS\")]:\n            target_peaks = {}\n            counter_peaks = {}\n            for mr in MAX_RULES_VALUES:\n                target_accs = [ds_data.get(target_method, {}).get(mr, {}).get(fold, {}).get(\"accuracy\", float(\"nan\"))\n                               for fold in range(N_FOLDS)]\n                counter_accs = [ds_data.get(counter_method, {}).get(mr, {}).get(fold, {}).get(\"accuracy\", float(\"nan\"))\n                                for fold in range(N_FOLDS)]\n                target_peaks[mr] = float(np.nanmean(target_accs))\n                counter_peaks[mr] = float(np.nanmean(counter_accs))\n            target_peak_mr = max(target_peaks, key=target_peaks.get)\n            counter_peak_mr = max(counter_peaks, key=counter_peaks.get)\n            if target_peak_mr < counter_peak_mr:\n                sg25_lower_count += 1\n            elif target_peak_mr > counter_peak_mr:\n                figs_lower_count += 1\n            else:\n                ties += 1\n\n    n_sign = sg25_lower_count + figs_lower_count\n    if n_sign > 0:\n        sign_test_p = float(stats.binomtest(sg25_lower_count, n_sign, 0.5).pvalue)\n    else:\n        sign_test_p = 1.0\n\n    results[\"peak_complexity_comparison\"] = {\n        \"sg25_peaks_lower\": sg25_lower_count,\n        \"figs_peaks_lower\": figs_lower_count,\n        \"ties\": ties,\n        \"sign_test_p\": sign_test_p,\n    }\n    return results\n\nblock5 = block5_complexity_curves(data=structured_data, datasets=datasets)\n\nprint(\"Block 5: Accuracy at Matched Complexity\")\nprint(f\"{'Method':<20} {'MR=5':>8} {'MR=10':>8} {'MR=15':>8} {'Peak':>6} {'Eff.Ratio':>10}\")\nprint(\"-\" * 62)\nfor m in METHODS:\n    b = block5[m]\n    print(f\"{m:<20} {b['accuracy_at_mr5']:>8.4f} {b['accuracy_at_mr10']:>8.4f} \"\n          f\"{b['accuracy_at_mr15']:>8.4f} {b['peak_max_rules']:>6.0f} {b['efficiency_ratio']:>10.4f}\")\npcc = block5[\"peak_complexity_comparison\"]\nprint(f\"\\nSign test: SG25 lower={pcc['sg25_peaks_lower']}, \"\n      f\"FIGS lower={pcc['figs_peaks_lower']}, ties={pcc['ties']}, p={pcc['sign_test_p']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kpn4x1qenhj",
   "source": "### Block 6: Ablation Decomposition\n\nDecomposes accuracy changes into `oblique_penalty` (FIGS - RO-FIGS) and `synergy_effect` (SG-FIGS-25 - RO-FIGS), classifying datasets into categories.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "99iz1716v3r",
   "source": "# ---------------------------------------------------------------------------\n# Block 6: Ablation Decomposition (from eval.py)\n# ---------------------------------------------------------------------------\n\ndef block6_ablation_decomposition(\n    data: dict,\n    datasets: list[str],\n    max_rules: int = 10,\n) -> dict:\n    \"\"\"Decompose accuracy changes: oblique_penalty and synergy_effect.\"\"\"\n    results = {}\n    categories_count = {\"oblique_harmful\": 0, \"synergy_helps\": 0, \"synergy_hurts\": 0, \"neutral\": 0}\n\n    for ds in datasets:\n        ds_data = data.get(ds, {})\n        means = {}\n        for method in [\"FIGS\", \"RO-FIGS\", \"SG-FIGS-25\"]:\n            mr_data = ds_data.get(method, {}).get(max_rules, {})\n            accs = [mr_data.get(fold, {}).get(\"accuracy\", float(\"nan\")) for fold in range(N_FOLDS)]\n            means[method] = float(np.nanmean(accs))\n\n        figs_acc = means[\"FIGS\"]\n        rofigs_acc = means[\"RO-FIGS\"]\n        sgfigs25_acc = means[\"SG-FIGS-25\"]\n\n        oblique_penalty = figs_acc - rofigs_acc\n        synergy_effect = sgfigs25_acc - rofigs_acc\n        total_gap = figs_acc - sgfigs25_acc\n\n        # Classify\n        category = \"neutral\"\n        if oblique_penalty > 0.05:\n            category = \"oblique_harmful\"\n        if synergy_effect > 0:\n            if category == \"oblique_harmful\":\n                category = \"oblique_harmful\"\n            else:\n                category = \"synergy_helps\"\n        elif synergy_effect < -0.05:\n            category = \"synergy_hurts\"\n\n        if category in categories_count:\n            categories_count[category] += 1\n\n        results[ds] = {\n            \"figs_accuracy\": figs_acc,\n            \"rofigs_accuracy\": rofigs_acc,\n            \"sgfigs25_accuracy\": sgfigs25_acc,\n            \"oblique_penalty\": oblique_penalty,\n            \"synergy_effect\": synergy_effect,\n            \"total_gap\": total_gap,\n            \"category\": category,\n        }\n\n    n_datasets = len(datasets)\n    category_fractions = {k: v / n_datasets for k, v in categories_count.items()}\n\n    return {\n        \"per_dataset\": results,\n        \"category_counts\": categories_count,\n        \"category_fractions\": category_fractions,\n    }\n\nblock6 = block6_ablation_decomposition(data=structured_data, datasets=datasets, max_rules=10)\n\nprint(\"Block 6: Ablation Decomposition\")\nprint(f\"{'Dataset':<15} {'FIGS':>8} {'RO-FIGS':>8} {'SG25':>8} {'OblPen':>8} {'SynEff':>8} {'Category':<18}\")\nprint(\"-\" * 85)\nfor ds in datasets:\n    b = block6[\"per_dataset\"][ds]\n    print(f\"{ds:<15} {b['figs_accuracy']:>8.4f} {b['rofigs_accuracy']:>8.4f} \"\n          f\"{b['sgfigs25_accuracy']:>8.4f} {b['oblique_penalty']:>+8.4f} \"\n          f\"{b['synergy_effect']:>+8.4f} {b['category']:<18}\")\nprint(f\"\\nCategory fractions: {block6['category_fractions']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "tliumc8o3o",
   "source": "### Aggregate Metrics Summary\n\nDisplay key aggregate metrics from the pre-computed `metrics_agg` dictionary.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "spn5prdkk7a",
   "source": "# Display pre-computed aggregate metrics from the data file\nmetrics = data[\"metrics_agg\"]\n\nprint(\"=\" * 60)\nprint(\"AGGREGATE METRICS SUMMARY\")\nprint(\"=\" * 60)\n\nprint(f\"\\nFriedman test: chi²={metrics['eval_friedman_chi_sq']:.2f}, \"\n      f\"p={metrics['eval_friedman_p']:.2e}\")\nprint(f\"Nemenyi CD: {metrics['eval_nemenyi_cd']:.3f}\")\n\nprint(\"\\nMethod Rankings (from full eval):\")\nprint(f\"  {'Method':<20} {'Mean Rank':>10} {'Grand Mean Acc':>15}\")\nprint(f\"  {'-'*47}\")\nfor m in METHODS:\n    safe = m.replace(\"-\", \"_\")\n    rank = metrics[f\"eval_mean_rank_{safe}\"]\n    acc = metrics[f\"eval_grand_mean_accuracy_{safe}\"]\n    print(f\"  {m:<20} {rank:>10.2f} {acc:>15.4f}\")\n\nprint(f\"\\nAblation fractions:\")\nfor cat in [\"oblique_harmful\", \"synergy_helps\", \"synergy_hurts\", \"neutral\"]:\n    frac = metrics[f\"eval_ablation_frac_{cat}\"]\n    print(f\"  {cat}: {frac:.1%}\")\n\nprint(f\"\\nPositive case studies: {int(metrics['eval_n_positive_case_studies'])}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0ukevrv6junb",
   "source": "### Visualization\n\nThree-panel figure: (1) Mean accuracy per method per dataset, (2) Accuracy-at-complexity curves, (3) Ablation decomposition.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "yl952f4kh",
   "source": "def visualize_results(block1_res, block5_res, block6_res, datasets_list, title_prefix=\"\"):\n    \"\"\"Reusable visualization function for evaluation results.\n\n    Creates a 3-panel figure:\n      1. Mean accuracy per method per dataset (grouped bar chart)\n      2. Accuracy-at-complexity curves (line plot across max_rules)\n      3. Ablation decomposition (stacked bar: oblique_penalty vs synergy_effect)\n    \"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n    colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \"#8c564b\"]\n    method_short = [\"FIGS\", \"RO-FIGS\", \"SG10\", \"SG25\", \"SG50\", \"GB\"]\n\n    # --- Panel 1: Grouped bar chart of mean accuracy ---\n    ax = axes[0]\n    x = np.arange(len(datasets_list))\n    width = 0.12\n    for j, method in enumerate(METHODS):\n        accs = [block1_res[ds][\"method_stats\"][method][\"accuracy_mean\"] for ds in datasets_list]\n        ax.bar(x + j * width - width * 2.5, accs, width, label=method_short[j], color=colors[j])\n    ax.set_xticks(x)\n    ax.set_xticklabels(datasets_list, rotation=45, ha=\"right\", fontsize=9)\n    ax.set_ylabel(\"Mean Accuracy\")\n    ax.set_title(f\"{title_prefix}Accuracy by Method & Dataset\")\n    ax.legend(fontsize=7, ncol=2)\n    ax.set_ylim(0.4, 1.05)\n\n    # --- Panel 2: Complexity curves ---\n    ax = axes[1]\n    mr_vals = MAX_RULES_VALUES\n    for j, method in enumerate(METHODS):\n        accs = [block5_res[method][f\"accuracy_at_mr{mr}\"] for mr in mr_vals]\n        ax.plot(mr_vals, accs, \"o-\", label=method_short[j], color=colors[j], linewidth=2)\n    ax.set_xlabel(\"max_rules\")\n    ax.set_ylabel(\"Mean Accuracy\")\n    ax.set_title(f\"{title_prefix}Accuracy vs Complexity\")\n    ax.legend(fontsize=7)\n    ax.set_xticks(mr_vals)\n\n    # --- Panel 3: Ablation decomposition ---\n    ax = axes[2]\n    ds_names = datasets_list\n    oblique_penalties = [block6_res[\"per_dataset\"][ds][\"oblique_penalty\"] for ds in ds_names]\n    synergy_effects = [block6_res[\"per_dataset\"][ds][\"synergy_effect\"] for ds in ds_names]\n    x = np.arange(len(ds_names))\n    ax.bar(x - 0.15, oblique_penalties, 0.3, label=\"Oblique Penalty\", color=\"#1f77b4\")\n    ax.bar(x + 0.15, synergy_effects, 0.3, label=\"Synergy Effect\", color=\"#2ca02c\")\n    ax.axhline(y=0, color=\"black\", linewidth=0.5)\n    ax.set_xticks(x)\n    ax.set_xticklabels(ds_names, rotation=45, ha=\"right\", fontsize=9)\n    ax.set_ylabel(\"Accuracy Difference\")\n    ax.set_title(f\"{title_prefix}Ablation Decomposition\")\n    ax.legend(fontsize=8)\n\n    plt.tight_layout()\n    plt.show()\n\n# Run visualization for Part 1 (mini data)\nvisualize_results(block1, block5, block6, datasets, title_prefix=\"[Mini] \")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0jn8x126v9ui",
   "source": "---\n## Part 2 — Full Run (Original Parameters)\n\nLoads the full dataset (all 12 datasets, 360 examples) and re-runs all analysis blocks with original parameters.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tmemq7gwqvg",
   "source": "# Load full dataset\ndata = load_full()\nprint(f\"Loaded full data: {len(data['datasets'])} datasets, \"\n      f\"{sum(len(d['examples']) for d in data['datasets'])} examples\")\nprint(f\"Datasets: {[d['dataset'] for d in data['datasets']]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ne1fikv8ft",
   "source": "# Restructure full data and run all blocks with original parameters\nstructured_data = restructure_eval_data(data)\ndatasets = list(structured_data.keys())\n\n# Block 1: Per-dataset accuracy (max_rules=10, all 12 datasets)\nblock1 = block1_per_dataset_accuracy(data=structured_data, datasets=datasets, max_rules=10)\n\nprint(\"Block 1: Per-Dataset Accuracy (max_rules=10) — Full Run\")\nprint(f\"{'Dataset':<15}\", end=\"\")\nfor m in METHODS:\n    print(f\"{m:>16}\", end=\"\")\nprint()\nprint(\"-\" * (15 + 16 * len(METHODS)))\nfor ds in datasets:\n    print(f\"{ds:<15}\", end=\"\")\n    for m in METHODS:\n        s = block1[ds][\"method_stats\"][m]\n        print(f\"  {s['accuracy_mean']:.3f}±{s['accuracy_std']:.3f}\", end=\"\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "lecg7l9h9e",
   "source": "# Block 2: Critical difference (Friedman + Nemenyi) — Full Run\nblock2 = block2_critical_difference(data=structured_data, datasets=datasets, max_rules=10)\n\nprint(f\"Friedman chi² = {block2['friedman_chi_sq']:.2f}, p = {block2['friedman_p']:.2e}\")\nprint(f\"Nemenyi CD = {block2['nemenyi_cd']:.3f}\")\nprint(\"\\nMean Ranks:\")\nfor m, r in sorted(block2[\"mean_ranks\"].items(), key=lambda x: x[1]):\n    print(f\"  {m:<20} {r:.2f}\")\nprint(f\"\\nCD Cliques: {block2['cd_cliques']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jorwr0b65xm",
   "source": "# Block 5: Complexity curves — Full Run\nblock5 = block5_complexity_curves(data=structured_data, datasets=datasets)\n\nprint(\"Block 5: Accuracy at Matched Complexity — Full Run\")\nprint(f\"{'Method':<20} {'MR=5':>8} {'MR=10':>8} {'MR=15':>8} {'Peak':>6} {'Eff.Ratio':>10}\")\nprint(\"-\" * 62)\nfor m in METHODS:\n    b = block5[m]\n    print(f\"{m:<20} {b['accuracy_at_mr5']:>8.4f} {b['accuracy_at_mr10']:>8.4f} \"\n          f\"{b['accuracy_at_mr15']:>8.4f} {b['peak_max_rules']:>6.0f} {b['efficiency_ratio']:>10.4f}\")\npcc = block5[\"peak_complexity_comparison\"]\nprint(f\"\\nSign test: SG25 lower={pcc['sg25_peaks_lower']}, \"\n      f\"FIGS lower={pcc['figs_peaks_lower']}, ties={pcc['ties']}, p={pcc['sign_test_p']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fnft8lx08yt",
   "source": "# Block 6: Ablation decomposition — Full Run\nblock6 = block6_ablation_decomposition(data=structured_data, datasets=datasets, max_rules=10)\n\nprint(\"Block 6: Ablation Decomposition — Full Run\")\nprint(f\"{'Dataset':<15} {'FIGS':>8} {'RO-FIGS':>8} {'SG25':>8} {'OblPen':>8} {'SynEff':>8} {'Category':<18}\")\nprint(\"-\" * 85)\nfor ds in datasets:\n    b = block6[\"per_dataset\"][ds]\n    print(f\"{ds:<15} {b['figs_accuracy']:>8.4f} {b['rofigs_accuracy']:>8.4f} \"\n          f\"{b['sgfigs25_accuracy']:>8.4f} {b['oblique_penalty']:>+8.4f} \"\n          f\"{b['synergy_effect']:>+8.4f} {b['category']:<18}\")\nprint(f\"\\nCategory fractions: {block6['category_fractions']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "xsfsieip4ad",
   "source": "# Full aggregate metrics summary\nmetrics = data[\"metrics_agg\"]\n\nprint(\"=\" * 60)\nprint(\"FULL AGGREGATE METRICS SUMMARY (12 datasets)\")\nprint(\"=\" * 60)\n\nprint(f\"\\nFriedman test: chi²={metrics['eval_friedman_chi_sq']:.2f}, \"\n      f\"p={metrics['eval_friedman_p']:.2e}\")\nprint(f\"Nemenyi CD: {metrics['eval_nemenyi_cd']:.3f}\")\n\nprint(\"\\nMethod Rankings:\")\nprint(f\"  {'Method':<20} {'Mean Rank':>10} {'Grand Mean Acc':>15}\")\nprint(f\"  {'-'*47}\")\nfor m in METHODS:\n    safe = m.replace(\"-\", \"_\")\n    rank = metrics[f\"eval_mean_rank_{safe}\"]\n    acc = metrics[f\"eval_grand_mean_accuracy_{safe}\"]\n    print(f\"  {m:<20} {rank:>10.2f} {acc:>15.4f}\")\n\nprint(f\"\\nAblation fractions:\")\nfor cat in [\"oblique_harmful\", \"synergy_helps\", \"synergy_hurts\", \"neutral\"]:\n    frac = metrics[f\"eval_ablation_frac_{cat}\"]\n    print(f\"  {cat}: {frac:.1%}\")\n\nprint(f\"\\nPositive case studies: {int(metrics['eval_n_positive_case_studies'])}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "er6y8f7in6k",
   "source": "# Full visualization — reuse the same function\nvisualize_results(block1, block5, block6, datasets, title_prefix=\"[Full] \")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}