{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "k3sl9me4w0c",
   "source": "# Synergy Graph Sufficiency Analysis\n\nEvaluates whether synergy graph structural properties (density, clique coverage, threshold sensitivity) explain the SG-FIGS accuracy gap across datasets.\n\n**5 Core Metrics:**\n1. Graph Density vs. Accuracy Gap Correlation\n2. Threshold Sensitivity Profile\n3. Oblique Activation Rate vs. Accuracy\n4. Sparse Graph Correction Counterfactual\n5. Stability-Performance Relationship\n\n**Part 1** runs a quick demo on a curated 5-dataset subset.\n**Part 2** runs the full analysis on all 12 datasets with original parameters.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hwiamqn0wgr",
   "source": "import json\nimport math\nimport os\n\nimport matplotlib.pyplot as plt",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "lhe5mjd1ttn",
   "source": "GITHUB_FULL_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-ac2586-synergy-guided-oblique-splits-using-part/main/synergygraph_ev/demo/full_demo_data.json\"\nGITHUB_MINI_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-ac2586-synergy-guided-oblique-splits-using-part/main/synergygraph_ev/demo/mini_demo_data.json\"\n\ndef _load_json(url, local_path):\n    try:\n        import urllib.request\n        with urllib.request.urlopen(url) as response:\n            return json.loads(response.read().decode())\n    except Exception: pass\n    if os.path.exists(local_path):\n        with open(local_path) as f: return json.load(f)\n    raise FileNotFoundError(f\"Could not load {local_path}\")\n\ndef load_mini():\n    return _load_json(GITHUB_MINI_DATA_URL, \"mini_demo_data.json\")\n\ndef load_full():\n    return _load_json(GITHUB_FULL_DATA_URL, \"full_demo_data.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "pumc5cx9vem",
   "source": "## Helper Functions\n\nStatistical helper functions copied from `eval.py` — Spearman/Pearson correlation with tied-rank handling and approximate p-values (scipy-free).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "sjx1oslohvd",
   "source": "def safe_float(val: float) -> float:\n    \"\"\"Convert NaN/Inf to None-safe float for JSON serialization.\"\"\"\n    if val is None or math.isnan(val) or math.isinf(val):\n        return 0.0\n    return float(val)\n\n\ndef _normal_cdf(x: float) -> float:\n    \"\"\"Approximate normal CDF using error function approximation.\"\"\"\n    return 0.5 * (1.0 + math.erf(x / math.sqrt(2)))\n\n\ndef spearman_correlation(x: list[float], y: list[float]) -> tuple[float, float]:\n    \"\"\"Compute Spearman rank correlation and approximate p-value.\n\n    Uses scipy-free implementation with tied-rank handling.\n    \"\"\"\n    n = len(x)\n    if n < 3:\n        return 0.0, 1.0\n\n    def rank_data(data: list[float]) -> list[float]:\n        \"\"\"Assign ranks with average tie-breaking.\"\"\"\n        indexed = sorted(enumerate(data), key=lambda t: t[1])\n        ranks = [0.0] * n\n        i = 0\n        while i < n:\n            j = i\n            while j < n - 1 and indexed[j + 1][1] == indexed[j][1]:\n                j += 1\n            avg_rank = (i + j) / 2.0 + 1.0\n            for k in range(i, j + 1):\n                ranks[indexed[k][0]] = avg_rank\n            i = j + 1\n        return ranks\n\n    rx = rank_data(x)\n    ry = rank_data(y)\n\n    # Pearson on ranks\n    mean_rx = sum(rx) / n\n    mean_ry = sum(ry) / n\n    num = sum((rx[i] - mean_rx) * (ry[i] - mean_ry) for i in range(n))\n    den_x = math.sqrt(sum((rx[i] - mean_rx) ** 2 for i in range(n)))\n    den_y = math.sqrt(sum((ry[i] - mean_ry) ** 2 for i in range(n)))\n\n    if den_x == 0 or den_y == 0:\n        return 0.0, 1.0\n\n    rho = num / (den_x * den_y)\n    rho = max(-1.0, min(1.0, rho))\n\n    # Approximate two-sided p-value using t-distribution approximation\n    if abs(rho) >= 1.0:\n        p_val = 0.0\n    else:\n        t_stat = rho * math.sqrt((n - 2) / (1 - rho**2))\n        p_val = 2 * (1 - _normal_cdf(abs(t_stat)))\n\n    return rho, p_val\n\n\ndef pearson_correlation(x: list[float], y: list[float]) -> tuple[float, float]:\n    \"\"\"Compute Pearson correlation and approximate p-value.\"\"\"\n    n = len(x)\n    if n < 3:\n        return 0.0, 1.0\n\n    mean_x = sum(x) / n\n    mean_y = sum(y) / n\n    num = sum((x[i] - mean_x) * (y[i] - mean_y) for i in range(n))\n    den_x = math.sqrt(sum((x[i] - mean_x) ** 2 for i in range(n)))\n    den_y = math.sqrt(sum((y[i] - mean_y) ** 2 for i in range(n)))\n\n    if den_x == 0 or den_y == 0:\n        return 0.0, 1.0\n\n    r = num / (den_x * den_y)\n    r = max(-1.0, min(1.0, r))\n\n    if abs(r) >= 1.0:\n        p_val = 0.0\n    else:\n        t_stat = r * math.sqrt((n - 2) / (1 - r**2))\n        p_val = 2 * (1 - _normal_cdf(abs(t_stat)))\n\n    return r, p_val",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "djt9agj484a",
   "source": "def extract_dataset_summaries(data: dict) -> list[dict]:\n    \"\"\"Extract one summary row per dataset from the synergy_graph_sufficiency examples.\"\"\"\n    summaries = []\n    for ds_block in data[\"datasets\"]:\n        # The first example per dataset is the summary (analysis=synergy_graph_sufficiency)\n        for ex in ds_block[\"examples\"]:\n            inp = json.loads(ex[\"input\"])\n            if inp.get(\"analysis\") == \"synergy_graph_sufficiency\":\n                summaries.append(ex)\n                break\n    return summaries\n\n\ndef extract_method_examples(data: dict) -> list[dict]:\n    \"\"\"Extract per-method performance examples.\"\"\"\n    methods = []\n    for ds_block in data[\"datasets\"]:\n        for ex in ds_block[\"examples\"]:\n            inp = json.loads(ex[\"input\"])\n            if inp.get(\"analysis\") == \"method_performance\":\n                methods.append(ex)\n    return methods\n\n\ndef run_analysis(data: dict) -> dict:\n    \"\"\"Run all 5 metrics analysis on loaded data. Returns dict of results.\"\"\"\n    summaries = extract_dataset_summaries(data)\n    method_examples = extract_method_examples(data)\n    metrics_agg = data[\"metrics_agg\"]\n    datasets = [json.loads(s[\"input\"])[\"dataset\"] for s in summaries]\n\n    # --- Metric 1: Graph Density vs Accuracy Gap ---\n    densities = [s.get(\"eval_graph_density_25pct\", 0) for s in summaries]\n    acc_gaps = [s.get(\"eval_accuracy_gap\", 0) for s in summaries]\n    rho1, p1 = spearman_correlation(densities, acc_gaps)\n\n    m1 = {\n        \"spearman_rho\": round(rho1, 4),\n        \"spearman_p_value\": round(p1, 6),\n        \"per_dataset\": {\n            json.loads(s[\"input\"])[\"dataset\"]: {\n                \"graph_density_25pct\": s.get(\"eval_graph_density_25pct\", 0),\n                \"accuracy_gap\": s.get(\"eval_accuracy_gap\", 0),\n                \"avg_synergy_edges\": s.get(\"eval_avg_synergy_edges\", 0),\n                \"n_features\": s.get(\"metadata_n_features\", 0),\n            }\n            for s in summaries\n        },\n    }\n\n    # --- Metric 2: Threshold Sensitivity ---\n    profile_counts = {\"monotonic_improving\": 0, \"monotonic_worsening\": 0, \"non_monotonic\": 0, \"flat\": 0}\n    m2_per_ds = {}\n    for s in summaries:\n        ds = json.loads(s[\"input\"])[\"dataset\"]\n        profile = s.get(\"metadata_threshold_profile\", \"unknown\")\n        if profile in profile_counts:\n            profile_counts[profile] += 1\n        m2_per_ds[ds] = {\n            \"figs_accuracy\": s.get(\"eval_figs_accuracy\", 0),\n            \"sg10_accuracy\": s.get(\"eval_sg10_accuracy\", 0),\n            \"sg25_accuracy\": s.get(\"eval_sg25_accuracy\", 0),\n            \"sg50_accuracy\": s.get(\"eval_sg50_accuracy\", 0),\n            \"profile\": profile,\n        }\n    m2 = {\"profile_distribution\": profile_counts, \"per_dataset\": m2_per_ds}\n\n    # --- Metric 3: Oblique Activation Rate ---\n    oblique_fracs = []\n    oblique_accs = []\n    zero_oblique = []\n    nonzero_oblique = []\n    for s in summaries:\n        ds = json.loads(s[\"input\"])[\"dataset\"]\n        of = s.get(\"eval_oblique_fraction_sg25\", 0)\n        acc = s.get(\"eval_accuracy_sg25\", 0)\n        oblique_fracs.append(of)\n        oblique_accs.append(acc)\n        if of == 0.0:\n            zero_oblique.append(ds)\n        else:\n            nonzero_oblique.append(ds)\n    r3, p3 = pearson_correlation(oblique_fracs, oblique_accs)\n    m3 = {\n        \"pearson_r\": round(r3, 4),\n        \"pearson_p_value\": round(p3, 6),\n        \"zero_oblique_datasets\": zero_oblique,\n        \"nonzero_oblique_datasets\": nonzero_oblique,\n    }\n\n    # --- Metric 4: Sparse Graph Correction ---\n    discrepancies = []\n    m4_per_ds = {}\n    for s in summaries:\n        ds = json.loads(s[\"input\"])[\"dataset\"]\n        if s.get(\"eval_oblique_fraction_sg25\", 0) == 0.0:\n            disc = s.get(\"eval_max_fold_discrepancy\", 0)\n            identical = s.get(\"eval_is_identical_to_figs\", 1.0) == 1.0\n            m4_per_ds[ds] = {\"max_fold_discrepancy\": disc, \"is_identical_to_figs\": identical}\n            if not identical:\n                discrepancies.append(ds)\n    m4 = {\"n_zero_oblique\": len(m4_per_ds), \"discrepancies\": discrepancies, \"per_dataset\": m4_per_ds}\n\n    # --- Metric 5: Stability-Performance ---\n    jaccards = []\n    stab_gaps = []\n    m5_per_ds = {}\n    for s in summaries:\n        ds = json.loads(s[\"input\"])[\"dataset\"]\n        j = s.get(\"eval_mean_jaccard\")\n        g = s.get(\"eval_stability_accuracy_gap\")\n        if j is not None and g is not None:\n            jaccards.append(j)\n            stab_gaps.append(g)\n            m5_per_ds[ds] = {\"mean_jaccard\": j, \"accuracy_gap\": g}\n    rho5, p5 = spearman_correlation(jaccards, stab_gaps)\n    m5 = {\"spearman_rho\": round(rho5, 4), \"spearman_p_value\": round(p5, 6), \"per_dataset\": m5_per_ds}\n\n    return {\n        \"metrics_agg\": metrics_agg,\n        \"metric1\": m1,\n        \"metric2\": m2,\n        \"metric3\": m3,\n        \"metric4\": m4,\n        \"metric5\": m5,\n        \"summaries\": summaries,\n        \"method_examples\": method_examples,\n        \"datasets\": datasets,\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0t61gjduqov",
   "source": "## Part 1 — Quick Demo (5-Dataset Subset)\n\nRuns on a curated 5-dataset subset: australian, banknote, breast_cancer, sonar, wine — covering all 4 threshold sensitivity profiles.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "beot6br7pu",
   "source": "data = load_mini()\nprint(f\"Loaded {len(data['datasets'])} datasets, \"\n      f\"{sum(len(d['examples']) for d in data['datasets'])} total examples\")\nprint(\"Datasets:\", [d[\"dataset\"] for d in data[\"datasets\"]])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kpqdrhd4vz",
   "source": "### Run All 5 Metrics\n\nExtract per-dataset summaries, compute Spearman/Pearson correlations, classify threshold profiles, and check sparse graph correction counterfactuals.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7r5by9j0a1h",
   "source": "results = run_analysis(data)\n\nprint(\"=\" * 60)\nprint(\"METRIC 1: Graph Density vs. Accuracy Gap\")\nprint(f\"  Spearman rho = {results['metric1']['spearman_rho']}, \"\n      f\"p = {results['metric1']['spearman_p_value']}\")\nprint()\nprint(\"METRIC 2: Threshold Sensitivity Profiles\")\nprint(f\"  Distribution: {results['metric2']['profile_distribution']}\")\nfor ds, info in results['metric2']['per_dataset'].items():\n    print(f\"    {ds:20s} FIGS={info['figs_accuracy']:.4f}  \"\n          f\"SG10={info['sg10_accuracy']:.4f}  SG25={info['sg25_accuracy']:.4f}  \"\n          f\"SG50={info['sg50_accuracy']:.4f}  -> {info['profile']}\")\nprint()\nprint(\"METRIC 3: Oblique Activation Rate\")\nprint(f\"  Pearson r = {results['metric3']['pearson_r']}, \"\n      f\"p = {results['metric3']['pearson_p_value']}\")\nprint(f\"  Zero-oblique datasets: {results['metric3']['zero_oblique_datasets']}\")\nprint(f\"  Nonzero-oblique datasets: {results['metric3']['nonzero_oblique_datasets']}\")\nprint()\nprint(\"METRIC 4: Sparse Graph Correction\")\nprint(f\"  Zero-oblique datasets checked: {results['metric4']['n_zero_oblique']}\")\nprint(f\"  Discrepancies from FIGS: {results['metric4']['discrepancies']}\")\nfor ds, info in results['metric4']['per_dataset'].items():\n    print(f\"    {ds:20s} max_discrepancy={info['max_fold_discrepancy']:.8f}  \"\n          f\"identical={info['is_identical_to_figs']}\")\nprint()\nprint(\"METRIC 5: Stability-Performance\")\nprint(f\"  Spearman rho = {results['metric5']['spearman_rho']}, \"\n      f\"p = {results['metric5']['spearman_p_value']}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ugujhpupn3q",
   "source": "### Visualization\n\nPlots: (1) Graph density vs accuracy gap scatter, (2) Threshold sensitivity bar chart across SG-FIGS thresholds, (3) Stability (Jaccard) vs accuracy gap.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dptcehmctqi",
   "source": "def visualize_results(results: dict, title_prefix: str = \"\") -> None:\n    \"\"\"Reusable visualization for synergy graph sufficiency analysis.\"\"\"\n    m1 = results[\"metric1\"]\n    m2 = results[\"metric2\"]\n    m5 = results[\"metric5\"]\n    datasets = results[\"datasets\"]\n\n    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n    # --- Plot 1: Graph Density vs Accuracy Gap ---\n    ax = axes[0]\n    densities = [m1[\"per_dataset\"][ds][\"graph_density_25pct\"] for ds in datasets if ds in m1[\"per_dataset\"]]\n    gaps = [m1[\"per_dataset\"][ds][\"accuracy_gap\"] for ds in datasets if ds in m1[\"per_dataset\"]]\n    ds_labels = [ds for ds in datasets if ds in m1[\"per_dataset\"]]\n    ax.scatter(densities, gaps, c=\"steelblue\", edgecolors=\"black\", s=80, zorder=3)\n    for i, ds in enumerate(ds_labels):\n        ax.annotate(ds, (densities[i], gaps[i]), fontsize=7, ha=\"center\", va=\"bottom\",\n                    xytext=(0, 5), textcoords=\"offset points\")\n    ax.set_xlabel(\"Graph Density (25% threshold)\")\n    ax.set_ylabel(\"Accuracy Gap (FIGS - SG-FIGS-25)\")\n    ax.set_title(f\"{title_prefix}Density vs Acc Gap\\n\"\n                 f\"rho={m1['spearman_rho']}, p={m1['spearman_p_value']:.4f}\")\n    ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n    ax.grid(alpha=0.3)\n\n    # --- Plot 2: Threshold Sensitivity ---\n    ax = axes[1]\n    ds_names = list(m2[\"per_dataset\"].keys())\n    x = range(len(ds_names))\n    figs_acc = [m2[\"per_dataset\"][ds][\"figs_accuracy\"] for ds in ds_names]\n    sg10_acc = [m2[\"per_dataset\"][ds][\"sg10_accuracy\"] for ds in ds_names]\n    sg25_acc = [m2[\"per_dataset\"][ds][\"sg25_accuracy\"] for ds in ds_names]\n    sg50_acc = [m2[\"per_dataset\"][ds][\"sg50_accuracy\"] for ds in ds_names]\n    width = 0.2\n    ax.bar([i - 1.5*width for i in x], figs_acc, width, label=\"FIGS\", color=\"gray\", alpha=0.8)\n    ax.bar([i - 0.5*width for i in x], sg10_acc, width, label=\"SG-10%\", color=\"lightcoral\")\n    ax.bar([i + 0.5*width for i in x], sg25_acc, width, label=\"SG-25%\", color=\"steelblue\")\n    ax.bar([i + 1.5*width for i in x], sg50_acc, width, label=\"SG-50%\", color=\"seagreen\")\n    ax.set_xticks(list(x))\n    ax.set_xticklabels(ds_names, rotation=45, ha=\"right\", fontsize=8)\n    ax.set_ylabel(\"Accuracy\")\n    ax.set_title(f\"{title_prefix}Threshold Sensitivity\")\n    ax.legend(fontsize=7, loc=\"lower left\")\n    ax.grid(axis=\"y\", alpha=0.3)\n\n    # --- Plot 3: Stability vs Accuracy Gap ---\n    ax = axes[2]\n    jaccards = [m5[\"per_dataset\"][ds][\"mean_jaccard\"] for ds in datasets if ds in m5[\"per_dataset\"]]\n    stab_gaps = [m5[\"per_dataset\"][ds][\"accuracy_gap\"] for ds in datasets if ds in m5[\"per_dataset\"]]\n    ds_labels5 = [ds for ds in datasets if ds in m5[\"per_dataset\"]]\n    ax.scatter(jaccards, stab_gaps, c=\"darkorange\", edgecolors=\"black\", s=80, zorder=3)\n    for i, ds in enumerate(ds_labels5):\n        ax.annotate(ds, (jaccards[i], stab_gaps[i]), fontsize=7, ha=\"center\", va=\"bottom\",\n                    xytext=(0, 5), textcoords=\"offset points\")\n    ax.set_xlabel(\"Mean Jaccard Stability\")\n    ax.set_ylabel(\"Accuracy Gap (FIGS - SG-FIGS-25)\")\n    ax.set_title(f\"{title_prefix}Stability vs Acc Gap\\n\"\n                 f\"rho={m5['spearman_rho']}, p={m5['spearman_p_value']:.4f}\")\n    ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n    ax.grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n\nvisualize_results(results, title_prefix=\"[Mini] \")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d6a1d06a7",
   "source": "## Full Run — Original Parameters\n\nLoads all 12 datasets and re-runs the same analysis with full data. All parameters match the original `eval.py` script.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gzsh4yzk3x9",
   "source": "data = load_full()\nprint(f\"Loaded {len(data['datasets'])} datasets, \"\n      f\"{sum(len(d['examples']) for d in data['datasets'])} total examples\")\nprint(\"Datasets:\", [d[\"dataset\"] for d in data[\"datasets\"]])\n\nresults_full = run_analysis(data)\n\nprint(\"=\" * 60)\nprint(\"METRIC 1: Graph Density vs. Accuracy Gap\")\nprint(f\"  Spearman rho = {results_full['metric1']['spearman_rho']}, \"\n      f\"p = {results_full['metric1']['spearman_p_value']}\")\nprint()\nprint(\"METRIC 2: Threshold Sensitivity Profiles\")\nprint(f\"  Distribution: {results_full['metric2']['profile_distribution']}\")\nfor ds, info in results_full['metric2']['per_dataset'].items():\n    print(f\"    {ds:20s} FIGS={info['figs_accuracy']:.4f}  \"\n          f\"SG10={info['sg10_accuracy']:.4f}  SG25={info['sg25_accuracy']:.4f}  \"\n          f\"SG50={info['sg50_accuracy']:.4f}  -> {info['profile']}\")\nprint()\nprint(\"METRIC 3: Oblique Activation Rate\")\nprint(f\"  Pearson r = {results_full['metric3']['pearson_r']}, \"\n      f\"p = {results_full['metric3']['pearson_p_value']}\")\nprint(f\"  Zero-oblique datasets: {results_full['metric3']['zero_oblique_datasets']}\")\nprint(f\"  Nonzero-oblique datasets: {results_full['metric3']['nonzero_oblique_datasets']}\")\nprint()\nprint(\"METRIC 4: Sparse Graph Correction\")\nprint(f\"  Zero-oblique datasets checked: {results_full['metric4']['n_zero_oblique']}\")\nprint(f\"  Discrepancies from FIGS: {results_full['metric4']['discrepancies']}\")\nfor ds, info in results_full['metric4']['per_dataset'].items():\n    print(f\"    {ds:20s} max_discrepancy={info['max_fold_discrepancy']:.8f}  \"\n          f\"identical={info['is_identical_to_figs']}\")\nprint()\nprint(\"METRIC 5: Stability-Performance\")\nprint(f\"  Spearman rho = {results_full['metric5']['spearman_rho']}, \"\n      f\"p = {results_full['metric5']['spearman_p_value']}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "tndp7fk8jv",
   "source": "visualize_results(results_full, title_prefix=\"[Full] \")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}