{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "whob7iiz3t",
   "source": "# SG-FIGS vs Baselines: Synergy-Guided Oblique Tree Ensembles\n\nThis notebook demonstrates the **SG-FIGS** (Synergy-Guided FIGS) methodology, which extends FIGS (Fast Interpretable Greedy-tree Sums) with oblique splits constrained by PID synergy analysis.\n\n**What this experiment does:**\n- Compares 4 methods: **SG-FIGS**, **RO-FIGS** (Random Oblique), standard **FIGS**, and **GradientBoosting**\n- Runs 5-fold cross-validation on tabular classification datasets\n- Measures accuracy, AUC, model complexity, and split interpretability score\n- SG-FIGS achieves 1.0 interpretability score with competitive accuracy\n\n**Part 1** runs a quick demo on 3 small datasets with reduced parameters.\n**Part 2** runs the (almost) full experiment on all 12 datasets with near-original parameters.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lkoifp45zbg",
   "source": "import json\nimport sys\nimport time\nfrom collections import Counter\n\nimport networkx as nx\nimport numpy as np\nfrom imodels import FIGSClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.preprocessing import KBinsDiscretizer, label_binarize\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Additional imports for notebook visualization\nimport matplotlib.pyplot as plt",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "td11izrnyre",
   "source": "GITHUB_FULL_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-ac2586-synergy-guided-oblique-splits-using-part/main/sg_figs_exp/demo/full_demo_data.json\"\nGITHUB_MINI_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-ac2586-synergy-guided-oblique-splits-using-part/main/sg_figs_exp/demo/mini_demo_data.json\"\nimport json, os\n\ndef _load_json(url, local_path):\n    try:\n        import urllib.request\n        with urllib.request.urlopen(url) as response:\n            return json.loads(response.read().decode())\n    except Exception: pass\n    if os.path.exists(local_path):\n        with open(local_path) as f: return json.load(f)\n    raise FileNotFoundError(f\"Could not load {local_path}\")\n\ndef load_mini():\n    return _load_json(GITHUB_MINI_DATA_URL, \"mini_demo_data.json\")\n\ndef load_full():\n    return _load_json(GITHUB_FULL_DATA_URL, \"full_demo_data.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xiy4srkrjge",
   "source": "## Part 1 — Quick Demo (Mini Data)\n\nLoad a curated subset of 3 datasets (wine, sonar, glass) with reduced parameters for a fast demo.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wp82xf9p94g",
   "source": "data = load_mini()\nprint(f\"Loaded {len(data['datasets'])} dataset entries:\")\nfor ds in data[\"datasets\"]:\n    print(f\"  {ds['dataset']}: {len(ds['examples'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d9a949lxav",
   "source": "### Phase 0 — Data Loading\n\nReconstruct X, y matrices from the JSON examples. Each example has an `input` (JSON-encoded feature dict), `output` (true label), and `metadata_fold` (cross-validation fold assignment).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cjqx6usu09",
   "source": "def load_all_datasets(\n    data: dict,\n    max_examples: int | None = None,\n) -> dict[str, dict]:\n    \"\"\"Load all datasets from JSON data dict.\n\n    Each dataset entry has structure:\n      {\"dataset\": str, \"examples\": [...]}\n\n    Returns dict: dataset_name -> {X, y, folds, feature_names, class_names, n_classes}\n    \"\"\"\n    all_datasets: dict[str, dict] = {}\n\n    # Filter to only regular datasets (skip fold_metrics, summary, synergy_graph_stats)\n    skip_datasets = {\"fold_metrics\", \"summary\", \"synergy_graph_stats\"}\n\n    for ds_entry in data[\"datasets\"]:\n        name = ds_entry[\"dataset\"]\n        if name in skip_datasets:\n            continue\n\n        examples = ds_entry[\"examples\"]\n\n        if max_examples is not None:\n            examples = examples[:max_examples]\n\n        # Get full feature names from first example\n        first_input = json.loads(examples[0][\"input\"])\n        feature_names = list(first_input.keys())\n        n_features = len(feature_names)\n        n_samples = len(examples)\n\n        # Reconstruct X, y, folds\n        X = np.zeros((n_samples, n_features))\n        y_labels: list[str] = []\n        folds = np.zeros(n_samples, dtype=int)\n\n        for idx, ex in enumerate(examples):\n            features = json.loads(ex[\"input\"])\n            X[idx] = [features[fname] for fname in feature_names]\n            y_labels.append(ex[\"output\"])\n            folds[idx] = ex[\"metadata_fold\"]\n\n        # Encode y as integers\n        unique_classes = sorted(set(y_labels))\n        class_to_int = {c: i for i, c in enumerate(unique_classes)}\n        y = np.array([class_to_int[c] for c in y_labels])\n\n        all_datasets[name] = {\n            \"X\": X,\n            \"y\": y,\n            \"folds\": folds,\n            \"feature_names\": feature_names,\n            \"class_names\": unique_classes,\n            \"n_classes\": len(unique_classes),\n        }\n        print(\n            f\"  {name}: {X.shape}, {len(unique_classes)} classes, \"\n            f\"{len(np.unique(folds))} folds\"\n        )\n\n    return all_datasets\n\n# --- Quick Demo: reduced parameters ---\nMAX_EXAMPLES = None  # Use all examples in the mini dataset\nMETHODS = [\"FIGS\", \"RO-FIGS\", \"SG-FIGS\", \"GradientBoosting\"]\nMAX_RULES_CANDIDATES = [3]  # Reduced from [5, 10, 15] for speed\n\ndatasets = load_all_datasets(data, max_examples=MAX_EXAMPLES)\nprint(f\"\\nLoaded {len(datasets)} datasets total\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cy0m2hhk2xe",
   "source": "### Phase 1 — PID Synergy Computation\n\nCompute pairwise Partial Information Decomposition (PID) synergy between features and the target variable. Build a synergy graph where edges connect feature pairs with above-threshold synergy, then extract candidate feature subsets (cliques) for oblique splits.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "byixvf06khl",
   "source": "def compute_mi(x_disc: np.ndarray, y_disc: np.ndarray) -> float:\n    \"\"\"Mutual information between two discrete arrays using counting.\"\"\"\n    n = len(x_disc)\n    if n == 0:\n        return 0.0\n    joint = Counter(zip(x_disc, y_disc))\n    px = Counter(x_disc)\n    py = Counter(y_disc)\n    mi = 0.0\n    for (xi, yi), count in joint.items():\n        pxy = count / n\n        pxi = px[xi] / n\n        pyi = py[yi] / n\n        if pxy > 0 and pxi > 0 and pyi > 0:\n            mi += pxy * np.log2(pxy / (pxi * pyi))\n    return max(float(mi), 0.0)\n\n\ndef compute_pid_synergy(\n    x1_disc: np.ndarray,\n    x2_disc: np.ndarray,\n    y_disc: np.ndarray,\n) -> float:\n    \"\"\"Williams-Beer I_min PID synergy between feature pair and target.\n\n    synergy = I(X1,X2 ; Y) - unique_1 - unique_2 - redundancy\n    where redundancy = min(I(X1;Y), I(X2;Y))  [I_min measure]\n    \"\"\"\n    mi_x1_y = compute_mi(x1_disc, y_disc)\n    mi_x2_y = compute_mi(x2_disc, y_disc)\n    # Joint MI: combine x1,x2 into single variable\n    joint_x = np.array([f\"{a}_{b}\" for a, b in zip(x1_disc, x2_disc)])\n    mi_joint_y = compute_mi(joint_x, y_disc)\n\n    redundancy = min(mi_x1_y, mi_x2_y)\n    unique_1 = mi_x1_y - redundancy  # >= 0 by construction\n    unique_2 = mi_x2_y - redundancy  # >= 0 by construction\n    synergy = mi_joint_y - unique_1 - unique_2 - redundancy\n    return max(float(synergy), 0.0)\n\n\ndef build_synergy_graph(\n    X: np.ndarray,\n    y: np.ndarray,\n    n_bins: int = 5,\n    percentile_threshold: int = 75,\n) -> tuple[nx.Graph, dict[tuple[int, int], float], float]:\n    \"\"\"Build synergy graph over features.\n\n    1. Discretize continuous features into n_bins equal-frequency bins\n    2. Compute pairwise PID synergy for all feature pairs\n    3. Build graph with edges for pairs above the threshold percentile\n\n    Returns: (graph, synergy_scores dict, threshold value)\n    \"\"\"\n    n_features = X.shape[1]\n\n    # Discretize (handle constant/near-constant features gracefully)\n    X_disc = np.zeros_like(X, dtype=int)\n    for f in range(n_features):\n        col = X[:, f]\n        n_unique = len(np.unique(col))\n        if n_unique <= 1:\n            X_disc[:, f] = 0\n        else:\n            actual_bins = min(n_bins, n_unique)\n            try:\n                kbd = KBinsDiscretizer(\n                    n_bins=actual_bins,\n                    encode=\"ordinal\",\n                    strategy=\"quantile\",\n                )\n                X_disc[:, f] = (\n                    kbd.fit_transform(col.reshape(-1, 1)).ravel().astype(int)\n                )\n            except ValueError:\n                X_disc[:, f] = 0\n\n    y_disc = y.astype(int)\n\n    # Compute all pairwise synergies\n    synergy_scores: dict[tuple[int, int], float] = {}\n    for i in range(n_features):\n        for j in range(i + 1, n_features):\n            synergy_scores[(i, j)] = compute_pid_synergy(\n                X_disc[:, i], X_disc[:, j], y_disc\n            )\n\n    # Build graph at threshold\n    all_syns = list(synergy_scores.values())\n    if not all_syns:\n        return nx.Graph(), synergy_scores, 0.0\n\n    threshold = float(np.percentile(all_syns, percentile_threshold))\n\n    # Ensure at least 3 edges even if threshold is high\n    edges_above = sum(1 for s in all_syns if s >= threshold)\n    if edges_above < 3 and len(all_syns) >= 3:\n        sorted_syns = sorted(all_syns, reverse=True)\n        threshold = sorted_syns[min(2, len(sorted_syns) - 1)]\n\n    G = nx.Graph()\n    G.add_nodes_from(range(n_features))\n    for (i, j), s in synergy_scores.items():\n        if s >= threshold:\n            G.add_edge(i, j, synergy=s)\n\n    return G, synergy_scores, threshold\n\n\ndef get_candidate_subsets(\n    G: nx.Graph,\n    max_clique_size: int = 4,\n) -> list[tuple[int, ...]]:\n    \"\"\"Extract candidate feature subsets from synergy graph.\n\n    Candidates = all edges (size-2) + all triangles (size-3) +\n                 all 4-cliques (size-4, if any)\n    \"\"\"\n    candidates: set[tuple[int, ...]] = set()\n\n    for clique in nx.enumerate_all_cliques(G):\n        if len(clique) < 2:\n            continue\n        if len(clique) > max_clique_size:\n            break\n        candidates.add(tuple(sorted(clique)))\n\n    return sorted(candidates)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "pezqu7nw9r",
   "source": "### Phase 2 — Oblique FIGS Implementation\n\nThe `ObliqueFIGSClassifier` extends FIGS with oblique (multi-feature) splits. Two modes:\n- **synergy**: uses feature subsets from the synergy graph cliques (SG-FIGS)\n- **random**: uses randomly sampled feature subsets (RO-FIGS)\n\nEach oblique split projects data onto a Ridge regression direction, then finds the best threshold via a decision stump.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xfusatasdb9",
   "source": "class ObliqueSplitNode:\n    \"\"\"A node in an oblique FIGS tree.\"\"\"\n\n    def __init__(self) -> None:\n        self.feature_indices: list[int] = []\n        self.weights: np.ndarray = np.array([])\n        self.threshold: float = 0.0\n        self.value: np.ndarray = np.array([])  # leaf value (n_classes,)\n        self.left: ObliqueSplitNode | None = None\n        self.right: ObliqueSplitNode | None = None\n        self.is_leaf: bool = True\n        self.impurity_reduction: float = 0.0\n        self.n_samples: int = 0\n\n\nclass ObliqueFIGSClassifier:\n    \"\"\"Oblique FIGS classifier with configurable feature subset selection.\n\n    Implements the FIGS greedy algorithm with oblique splits:\n    1. Start with root nodes as a pool of potential splits\n    2. Greedily pick the split with highest impurity reduction\n    3. Add its children to the pool\n    4. Update residuals (subtract predictions from other trees)\n    5. Repeat until max_rules reached\n    \"\"\"\n\n    def __init__(\n        self,\n        max_rules: int = 10,\n        candidate_mode: str = \"synergy\",\n        synergy_graph: nx.Graph | None = None,\n        synergy_candidates: list[tuple[int, ...]] | None = None,\n        beam_size: int = 5,\n        subset_size_range: tuple[int, int] = (2, 4),\n        random_state: int = 42,\n    ) -> None:\n        self.max_rules = max_rules\n        self.candidate_mode = candidate_mode\n        self.synergy_graph = synergy_graph\n        self.synergy_candidates = synergy_candidates or []\n        self.beam_size = beam_size\n        self.subset_size_range = subset_size_range\n        self.random_state = random_state\n        self.trees_: list[ObliqueSplitNode] = []\n        self.complexity_: int = 0\n        self.n_classes_: int = 0\n        self.classes_: np.ndarray = np.array([])\n\n    def _get_candidate_subsets(\n        self,\n        n_features: int,\n        rng: np.random.RandomState,\n    ) -> list[tuple[int, ...]]:\n        \"\"\"Get feature subsets to evaluate for oblique split.\"\"\"\n        if self.candidate_mode == \"synergy\":\n            candidates = list(self.synergy_candidates)\n            if len(candidates) > 50:\n                idx = rng.choice(len(candidates), size=50, replace=False)\n                candidates = [candidates[i] for i in idx]\n            return candidates\n        elif self.candidate_mode == \"random\":\n            candidates: list[tuple[int, ...]] = []\n            for _ in range(self.beam_size):\n                size = rng.randint(\n                    self.subset_size_range[0],\n                    self.subset_size_range[1] + 1,\n                )\n                size = min(size, n_features)\n                subset = tuple(sorted(rng.choice(n_features, size=size, replace=False)))\n                candidates.append(subset)\n            return candidates\n        else:\n            raise ValueError(f\"Unknown candidate_mode: {self.candidate_mode}\")\n\n    def _find_best_oblique_split(\n        self,\n        X: np.ndarray,\n        y_residuals: np.ndarray,\n        idxs: np.ndarray,\n        candidates: list[tuple[int, ...]],\n    ) -> dict | None:\n        \"\"\"Find best oblique split among candidate feature subsets.\"\"\"\n        X_node = X[idxs]\n        y_node = y_residuals[idxs]\n\n        if X_node.shape[0] < 4:\n            return None\n\n        n_outputs = y_node.shape[1] if y_node.ndim > 1 else 1\n        if y_node.ndim == 1:\n            y_node = y_node.reshape(-1, 1)\n\n        best: dict | None = None\n        best_imp_red = -np.inf\n\n        for subset in candidates:\n            subset_list = list(subset)\n            X_sub = X_node[:, subset_list]\n\n            if X_sub.shape[0] < 4:\n                continue\n\n            for c in range(n_outputs):\n                y_target_col = y_node[:, c]\n\n                if np.std(y_target_col) < 1e-10:\n                    continue\n\n                try:\n                    ridge = Ridge(alpha=1.0)\n                    ridge.fit(X_sub, y_target_col)\n                except Exception:\n                    continue\n\n                weights = ridge.coef_\n                if np.all(np.abs(weights) < 1e-10):\n                    continue\n\n                proj_node = X_node[:, subset_list] @ weights\n\n                try:\n                    stump = DecisionTreeRegressor(max_depth=1)\n                    stump.fit(proj_node.reshape(-1, 1), y_node)\n                except Exception:\n                    continue\n\n                if stump.tree_.feature[0] < 0:\n                    continue\n\n                imp = stump.tree_.impurity\n                ns = stump.tree_.n_node_samples\n                if len(imp) < 3:\n                    continue\n\n                imp_red = (\n                    imp[0] - imp[1] * ns[1] / ns[0] - imp[2] * ns[2] / ns[0]\n                ) * ns[0]\n\n                if imp_red > best_imp_red:\n                    best_imp_red = imp_red\n                    threshold_val = stump.tree_.threshold[0]\n\n                    proj_all = X[:, subset_list] @ weights\n                    left_mask = np.zeros(len(X), dtype=bool)\n                    right_mask = np.zeros(len(X), dtype=bool)\n                    for ii in np.where(idxs)[0]:\n                        if proj_all[ii] <= threshold_val:\n                            left_mask[ii] = True\n                        else:\n                            right_mask[ii] = True\n\n                    left_val = stump.tree_.value[1].flatten()\n                    right_val = stump.tree_.value[2].flatten()\n\n                    best = {\n                        \"feature_indices\": subset_list,\n                        \"weights\": weights.copy(),\n                        \"threshold\": threshold_val,\n                        \"impurity_reduction\": imp_red,\n                        \"left_value\": left_val.copy(),\n                        \"right_value\": right_val.copy(),\n                        \"left_idxs\": left_mask.copy(),\n                        \"right_idxs\": right_mask.copy(),\n                    }\n\n        # Also evaluate axis-aligned split as fallback\n        try:\n            stump_aa = DecisionTreeRegressor(max_depth=1)\n            stump_aa.fit(X_node, y_node)\n            if stump_aa.tree_.feature[0] >= 0 and len(stump_aa.tree_.impurity) >= 3:\n                imp_aa = stump_aa.tree_.impurity\n                ns_aa = stump_aa.tree_.n_node_samples\n                imp_red_aa = (\n                    imp_aa[0]\n                    - imp_aa[1] * ns_aa[1] / ns_aa[0]\n                    - imp_aa[2] * ns_aa[2] / ns_aa[0]\n                ) * ns_aa[0]\n\n                if imp_red_aa > best_imp_red:\n                    feat = stump_aa.tree_.feature[0]\n                    thresh = stump_aa.tree_.threshold[0]\n\n                    left_mask = np.zeros(len(X), dtype=bool)\n                    right_mask = np.zeros(len(X), dtype=bool)\n                    for ii in np.where(idxs)[0]:\n                        if X[ii, feat] <= thresh:\n                            left_mask[ii] = True\n                        else:\n                            right_mask[ii] = True\n\n                    best = {\n                        \"feature_indices\": [feat],\n                        \"weights\": np.array([1.0]),\n                        \"threshold\": thresh,\n                        \"impurity_reduction\": imp_red_aa,\n                        \"left_value\": stump_aa.tree_.value[1].flatten().copy(),\n                        \"right_value\": stump_aa.tree_.value[2].flatten().copy(),\n                        \"left_idxs\": left_mask.copy(),\n                        \"right_idxs\": right_mask.copy(),\n                    }\n        except Exception:\n            pass\n\n        return best\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -> \"ObliqueFIGSClassifier\":\n        \"\"\"Fit ObliqueFIGS following the FIGS greedy algorithm.\"\"\"\n        rng = np.random.RandomState(self.random_state)\n        self.classes_ = np.unique(y)\n        self.n_classes_ = len(self.classes_)\n        n_samples, n_features = X.shape\n\n        Y_onehot = np.zeros((n_samples, self.n_classes_))\n        for i, c in enumerate(self.classes_):\n            Y_onehot[y == c, i] = 1.0\n\n        candidates = self._get_candidate_subsets(n_features, rng)\n        if not candidates:\n            for _ in range(5):\n                size = min(rng.randint(2, 4), n_features)\n                s = tuple(sorted(rng.choice(n_features, size=size, replace=False)))\n                candidates.append(s)\n\n        root = ObliqueSplitNode()\n        root.is_leaf = True\n        all_idxs = np.ones(n_samples, dtype=bool)\n        root.value = Y_onehot[all_idxs].mean(axis=0)\n        root.n_samples = n_samples\n\n        self.trees_ = [root]\n        pool: list[tuple[int, ObliqueSplitNode, np.ndarray]] = [\n            (0, root, all_idxs.copy())\n        ]\n\n        total_splits = 0\n\n        for _step in range(self.max_rules):\n            if not pool:\n                break\n\n            preds = np.zeros((n_samples, self.n_classes_))\n            for tree_root in self.trees_:\n                preds += self._predict_tree(X, tree_root)\n\n            residuals = Y_onehot - preds\n\n            best_split_info = None\n            best_pool_idx = -1\n            best_imp = -np.inf\n\n            for pidx, (tree_idx, node, node_idxs) in enumerate(pool):\n                if np.sum(node_idxs) < 4:\n                    continue\n\n                split_info = self._find_best_oblique_split(\n                    X, residuals, node_idxs, candidates\n                )\n                if split_info is not None and split_info[\"impurity_reduction\"] > best_imp:\n                    best_imp = split_info[\"impurity_reduction\"]\n                    best_split_info = split_info\n                    best_pool_idx = pidx\n\n            if best_split_info is None or best_imp <= 0:\n                break\n\n            tree_idx, node, node_idxs = pool[best_pool_idx]\n            pool.pop(best_pool_idx)\n\n            node.is_leaf = False\n            node.feature_indices = best_split_info[\"feature_indices\"]\n            node.weights = best_split_info[\"weights\"]\n            node.threshold = best_split_info[\"threshold\"]\n            node.impurity_reduction = best_split_info[\"impurity_reduction\"]\n\n            left_idxs = best_split_info[\"left_idxs\"]\n            right_idxs = best_split_info[\"right_idxs\"]\n\n            left_child = ObliqueSplitNode()\n            left_child.is_leaf = True\n            n_left = np.sum(left_idxs)\n            left_child.value = (\n                residuals[left_idxs].mean(axis=0) if n_left > 0 else np.zeros(self.n_classes_)\n            )\n            left_child.n_samples = int(n_left)\n\n            right_child = ObliqueSplitNode()\n            right_child.is_leaf = True\n            n_right = np.sum(right_idxs)\n            right_child.value = (\n                residuals[right_idxs].mean(axis=0) if n_right > 0 else np.zeros(self.n_classes_)\n            )\n            right_child.n_samples = int(n_right)\n\n            node.left = left_child\n            node.right = right_child\n\n            total_splits += 1\n\n            if n_left >= 4:\n                pool.append((tree_idx, left_child, left_idxs))\n            if n_right >= 4:\n                pool.append((tree_idx, right_child, right_idxs))\n\n            if total_splits % 3 == 0 and total_splits < self.max_rules:\n                new_root = ObliqueSplitNode()\n                new_root.is_leaf = True\n                new_root.value = residuals.mean(axis=0)\n                new_root.n_samples = n_samples\n                new_tree_idx = len(self.trees_)\n                self.trees_.append(new_root)\n                pool.append((new_tree_idx, new_root, np.ones(n_samples, dtype=bool)))\n\n        self.complexity_ = total_splits\n        return self\n\n    def _predict_tree(self, X: np.ndarray, node: ObliqueSplitNode) -> np.ndarray:\n        \"\"\"Predict from a single tree, returning (n_samples, n_classes) values.\"\"\"\n        n_samples = X.shape[0]\n        result = np.zeros((n_samples, self.n_classes_))\n\n        def _recurse(\n            nd: ObliqueSplitNode,\n            sample_mask: np.ndarray,\n        ) -> None:\n            if nd.is_leaf:\n                result[sample_mask] += nd.value\n                return\n\n            feat_idx = nd.feature_indices\n            proj = X[sample_mask][:, feat_idx] @ nd.weights\n\n            left_local = proj <= nd.threshold\n            right_local = ~left_local\n\n            full_indices = np.where(sample_mask)[0]\n            left_mask = np.zeros(n_samples, dtype=bool)\n            right_mask = np.zeros(n_samples, dtype=bool)\n            left_mask[full_indices[left_local]] = True\n            right_mask[full_indices[right_local]] = True\n\n            if nd.left is not None and np.any(left_mask):\n                _recurse(nd.left, left_mask)\n            if nd.right is not None and np.any(right_mask):\n                _recurse(nd.right, right_mask)\n\n        all_mask = np.ones(n_samples, dtype=bool)\n        _recurse(node, all_mask)\n        return result\n\n    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Return class probabilities via softmax of summed tree outputs.\"\"\"\n        n_samples = X.shape[0]\n        raw = np.zeros((n_samples, self.n_classes_))\n        for tree_root in self.trees_:\n            raw += self._predict_tree(X, tree_root)\n\n        raw_shifted = raw - raw.max(axis=1, keepdims=True)\n        exp_raw = np.exp(raw_shifted)\n        proba = exp_raw / exp_raw.sum(axis=1, keepdims=True)\n        return proba\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Predict by summing tree outputs, then argmax.\"\"\"\n        proba = self.predict_proba(X)\n        return self.classes_[np.argmax(proba, axis=1)]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bms53xd9fh7",
   "source": "### Phase 3 — Evaluation Helpers\n\nHelper functions for computing split interpretability scores, model complexity metrics, and AUC.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "q8mt9excfmd",
   "source": "def traverse_tree(node: ObliqueSplitNode) -> list[ObliqueSplitNode]:\n    \"\"\"Traverse tree and return all nodes.\"\"\"\n    nodes = [node]\n    if node.left is not None:\n        nodes.extend(traverse_tree(node.left))\n    if node.right is not None:\n        nodes.extend(traverse_tree(node.right))\n    return nodes\n\n\ndef compute_split_interpretability_score(\n    model: ObliqueFIGSClassifier,\n    synergy_scores: dict[tuple[int, int], float],\n    median_synergy: float,\n) -> float:\n    \"\"\"Fraction of oblique splits whose feature pairs have above-median synergy.\n\n    Axis-aligned splits (1 feature): counted as 1.0 (trivially interpretable).\n    \"\"\"\n    n_interpretable = 0\n    n_total = 0\n    for tree in model.trees_:\n        for node in traverse_tree(tree):\n            if node.is_leaf:\n                continue\n            n_total += 1\n            if len(node.feature_indices) == 1:\n                n_interpretable += 1\n            else:\n                pairs_above = 0\n                total_pairs = 0\n                for i in range(len(node.feature_indices)):\n                    for j in range(i + 1, len(node.feature_indices)):\n                        fi, fj = sorted(\n                            [node.feature_indices[i], node.feature_indices[j]]\n                        )\n                        total_pairs += 1\n                        if synergy_scores.get((fi, fj), 0) >= median_synergy:\n                            pairs_above += 1\n                if total_pairs > 0 and pairs_above == total_pairs:\n                    n_interpretable += 1\n    return n_interpretable / max(n_total, 1)\n\n\ndef compute_mean_features_per_split(model: ObliqueFIGSClassifier) -> float:\n    \"\"\"Average number of features used per split.\"\"\"\n    counts = []\n    for tree in model.trees_:\n        for node in traverse_tree(tree):\n            if not node.is_leaf:\n                counts.append(len(node.feature_indices))\n    return float(np.mean(counts)) if counts else 0.0\n\n\ndef compute_n_splits(model: ObliqueFIGSClassifier) -> int:\n    \"\"\"Total number of splits across all trees.\"\"\"\n    n = 0\n    for tree in model.trees_:\n        for node in traverse_tree(tree):\n            if not node.is_leaf:\n                n += 1\n    return n\n\n\ndef safe_auc(\n    y_true: np.ndarray,\n    y_proba: np.ndarray,\n    n_classes: int,\n    classes: np.ndarray,\n) -> float:\n    \"\"\"Compute AUC safely, handling edge cases.\"\"\"\n    try:\n        if n_classes == 2:\n            if y_proba.ndim == 2 and y_proba.shape[1] >= 2:\n                return float(roc_auc_score(y_true, y_proba[:, 1]))\n            else:\n                return float(\"nan\")\n        else:\n            present = np.unique(y_true)\n            if len(present) < 2:\n                return float(\"nan\")\n            y_bin = label_binarize(y_true, classes=classes)\n            return float(\n                roc_auc_score(y_bin, y_proba, multi_class=\"ovr\", average=\"macro\")\n            )\n    except Exception:\n        return float(\"nan\")\n\n\ndef count_figs_splits(model: FIGSClassifier) -> int:\n    \"\"\"Count total splits in an imodels FIGS model.\"\"\"\n    n = 0\n    for tree in model.trees_:\n        stack = [tree]\n        while stack:\n            node = stack.pop()\n            if node.left is not None or node.right is not None:\n                n += 1\n            if node.left is not None:\n                stack.append(node.left)\n            if node.right is not None:\n                stack.append(node.right)\n    return n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xw4iiug1sjd",
   "source": "### Phase 4 — Run Experiment\n\nTrain and evaluate all 4 methods on each dataset using 5-fold cross-validation. Collect per-fold metrics and per-sample predictions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5ns56efvdew",
   "source": "def run_experiment(datasets, max_rules_candidates, methods, gb_n_estimators=100):\n    \"\"\"Main experiment runner. Returns (summary, fold_metrics, dataset_order).\"\"\"\n    t_start = time.time()\n\n    synergy_graph_stats: dict[str, dict] = {}\n    fold_metrics: list[dict] = []\n\n    # Sort by dataset size (smallest first)\n    dataset_order = sorted(datasets.keys(), key=lambda d: datasets[d][\"X\"].shape[0])\n    print(f\"Dataset order: {dataset_order}\")\n\n    for ds_idx, ds_name in enumerate(dataset_order):\n        ds = datasets[ds_name]\n        X, y, folds = ds[\"X\"], ds[\"y\"], ds[\"folds\"]\n        feature_names = ds[\"feature_names\"]\n        class_names = ds[\"class_names\"]\n        n_classes = ds[\"n_classes\"]\n        classes_arr = np.arange(n_classes)\n\n        print(\n            f\"[{ds_idx+1}/{len(datasets)}] Processing {ds_name}: \"\n            f\"{X.shape}, {n_classes} classes\"\n        )\n\n        # Phase 1: Build synergy graph (once per dataset)\n        t_syn = time.time()\n        G, synergy_scores, syn_threshold = build_synergy_graph(X, y)\n        candidates = get_candidate_subsets(G)\n        all_syns = list(synergy_scores.values())\n        median_synergy = float(np.median(all_syns)) if all_syns else 0.0\n        syn_time = time.time() - t_syn\n\n        synergy_graph_stats[ds_name] = {\n            \"n_edges\": G.number_of_edges(),\n            \"n_candidates\": len(candidates),\n            \"density\": float(nx.density(G)),\n            \"threshold\": float(syn_threshold),\n            \"median_synergy\": median_synergy,\n            \"synergy_computation_time_seconds\": round(syn_time, 2),\n        }\n\n        print(\n            f\"  Synergy graph: {G.number_of_edges()} edges, \"\n            f\"{len(candidates)} candidates, time={syn_time:.1f}s\"\n        )\n\n        # 5-fold CV using pre-assigned folds\n        unique_folds = sorted(np.unique(folds))\n\n        for fold_id in unique_folds:\n            test_mask = folds == fold_id\n            train_mask = ~test_mask\n            X_train, X_test = X[train_mask], X[test_mask]\n            y_train, y_test = y[train_mask], y[test_mask]\n\n            if len(X_train) < 4 or len(X_test) < 1:\n                continue\n\n            # ---- Method A: Standard FIGS (axis-aligned) ----\n            best_figs_acc = -1.0\n            best_figs_metrics: dict = {}\n            best_max_rules = max_rules_candidates[0]\n\n            for max_rules in max_rules_candidates:\n                try:\n                    figs = FIGSClassifier(max_rules=max_rules)\n                    t0 = time.time()\n                    figs.fit(X_train, y_train)\n                    train_time = time.time() - t0\n\n                    y_pred = figs.predict(X_test)\n                    acc = float(accuracy_score(y_test, y_pred))\n                    proba = figs.predict_proba(X_test)\n                    auc_val = safe_auc(\n                        y_true=y_test, y_proba=proba,\n                        n_classes=n_classes, classes=classes_arr,\n                    )\n                    n_splits = count_figs_splits(figs)\n\n                    if acc > best_figs_acc:\n                        best_figs_acc = acc\n                        best_max_rules = max_rules\n                        best_figs_metrics = {\n                            \"accuracy\": round(acc, 6),\n                            \"auc\": round(auc_val, 6) if not np.isnan(auc_val) else None,\n                            \"n_splits\": n_splits,\n                            \"mean_features_per_split\": 1.0,\n                            \"split_interpretability_score\": 1.0,\n                            \"train_time_seconds\": round(train_time, 4),\n                        }\n                except Exception:\n                    pass\n\n            if best_figs_metrics:\n                fold_metrics.append({\n                    \"dataset\": ds_name, \"fold\": int(fold_id), \"method\": \"FIGS\",\n                    **best_figs_metrics,\n                })\n\n            # ---- Method B: RO-FIGS (random oblique) ----\n            try:\n                ro_figs = ObliqueFIGSClassifier(\n                    max_rules=best_max_rules,\n                    candidate_mode=\"random\",\n                    beam_size=5,\n                    subset_size_range=(2, 4),\n                    random_state=42,\n                )\n                t0 = time.time()\n                ro_figs.fit(X_train, y_train)\n                train_time = time.time() - t0\n\n                y_pred = ro_figs.predict(X_test)\n                acc = float(accuracy_score(y_test, y_pred))\n                proba = ro_figs.predict_proba(X_test)\n                auc_val = safe_auc(\n                    y_true=y_test, y_proba=proba,\n                    n_classes=n_classes, classes=classes_arr,\n                )\n\n                fold_metrics.append({\n                    \"dataset\": ds_name, \"fold\": int(fold_id), \"method\": \"RO-FIGS\",\n                    \"accuracy\": round(acc, 6),\n                    \"auc\": round(auc_val, 6) if not np.isnan(auc_val) else None,\n                    \"n_splits\": compute_n_splits(ro_figs),\n                    \"mean_features_per_split\": round(\n                        compute_mean_features_per_split(ro_figs), 3\n                    ),\n                    \"split_interpretability_score\": round(\n                        compute_split_interpretability_score(\n                            ro_figs, synergy_scores, median_synergy\n                        ), 4,\n                    ),\n                    \"train_time_seconds\": round(train_time, 4),\n                })\n            except Exception:\n                pass\n\n            # ---- Method C: SG-FIGS (synergy-guided oblique) ----\n            try:\n                sg_figs = ObliqueFIGSClassifier(\n                    max_rules=best_max_rules,\n                    candidate_mode=\"synergy\",\n                    synergy_graph=G,\n                    synergy_candidates=candidates,\n                    random_state=42,\n                )\n                t0 = time.time()\n                sg_figs.fit(X_train, y_train)\n                train_time = time.time() - t0\n\n                y_pred = sg_figs.predict(X_test)\n                acc = float(accuracy_score(y_test, y_pred))\n                proba = sg_figs.predict_proba(X_test)\n                auc_val = safe_auc(\n                    y_true=y_test, y_proba=proba,\n                    n_classes=n_classes, classes=classes_arr,\n                )\n\n                fold_metrics.append({\n                    \"dataset\": ds_name, \"fold\": int(fold_id), \"method\": \"SG-FIGS\",\n                    \"accuracy\": round(acc, 6),\n                    \"auc\": round(auc_val, 6) if not np.isnan(auc_val) else None,\n                    \"n_splits\": compute_n_splits(sg_figs),\n                    \"mean_features_per_split\": round(\n                        compute_mean_features_per_split(sg_figs), 3\n                    ),\n                    \"split_interpretability_score\": round(\n                        compute_split_interpretability_score(\n                            sg_figs, synergy_scores, median_synergy\n                        ), 4,\n                    ),\n                    \"train_time_seconds\": round(train_time, 4),\n                })\n            except Exception:\n                pass\n\n            # ---- Method D: GradientBoosting baseline ----\n            try:\n                gb = GradientBoostingClassifier(\n                    n_estimators=gb_n_estimators,\n                    max_depth=3,\n                    random_state=42,\n                )\n                t0 = time.time()\n                gb.fit(X_train, y_train)\n                train_time = time.time() - t0\n\n                y_pred = gb.predict(X_test)\n                acc = float(accuracy_score(y_test, y_pred))\n                proba = gb.predict_proba(X_test)\n                auc_val = safe_auc(\n                    y_true=y_test, y_proba=proba,\n                    n_classes=n_classes, classes=classes_arr,\n                )\n\n                fold_metrics.append({\n                    \"dataset\": ds_name, \"fold\": int(fold_id), \"method\": \"GradientBoosting\",\n                    \"accuracy\": round(acc, 6),\n                    \"auc\": round(auc_val, 6) if not np.isnan(auc_val) else None,\n                    \"n_splits\": gb_n_estimators * (2**3 - 1),\n                    \"mean_features_per_split\": 1.0,\n                    \"split_interpretability_score\": None,\n                    \"train_time_seconds\": round(train_time, 4),\n                })\n            except Exception:\n                pass\n\n    total_runtime = time.time() - t_start\n\n    # ---- Build summary: mean +/- std across folds ----\n    summary: list[dict] = []\n    for ds_name in dataset_order:\n        for method in methods:\n            rows = [\n                r for r in fold_metrics\n                if r[\"dataset\"] == ds_name and r[\"method\"] == method\n            ]\n            if not rows:\n                continue\n\n            accs = [r[\"accuracy\"] for r in rows]\n            aucs = [r[\"auc\"] for r in rows if r[\"auc\"] is not None]\n            n_splits_list = [r[\"n_splits\"] for r in rows]\n            interp_list = [\n                r[\"split_interpretability_score\"]\n                for r in rows\n                if r[\"split_interpretability_score\"] is not None\n            ]\n            mfps_list = [r[\"mean_features_per_split\"] for r in rows]\n            times_list = [r[\"train_time_seconds\"] for r in rows]\n\n            summary.append({\n                \"dataset\": ds_name,\n                \"method\": method,\n                \"accuracy_mean\": round(float(np.mean(accs)), 6),\n                \"accuracy_std\": round(float(np.std(accs)), 6),\n                \"auc_mean\": round(float(np.mean(aucs)), 6) if aucs else None,\n                \"auc_std\": round(float(np.std(aucs)), 6) if aucs else None,\n                \"n_splits_mean\": round(float(np.mean(n_splits_list)), 2),\n                \"interpretability_mean\": round(float(np.mean(interp_list)), 4) if interp_list else None,\n                \"train_time_mean\": round(float(np.mean(times_list)), 4),\n            })\n\n    print(f\"\\nTotal runtime: {total_runtime:.1f}s\")\n    return summary, fold_metrics, dataset_order\n\n# Run Part 1 with reduced params\nsummary_mini, fold_metrics_mini, dataset_order_mini = run_experiment(\n    datasets=datasets,\n    max_rules_candidates=MAX_RULES_CANDIDATES,  # [3] — reduced\n    methods=METHODS,\n    gb_n_estimators=20,  # Reduced from 100\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cytcf5aja7n",
   "source": "### Results Visualization\n\nDisplay summary table and accuracy comparison plot across methods and datasets.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "l2v70yhsa6",
   "source": "def visualize_results(summary, dataset_order, methods, title_suffix=\"\"):\n    \"\"\"Display summary table and accuracy comparison bar chart.\"\"\"\n\n    # ---- Print summary table ----\n    print(\"=\" * 70)\n    print(f\"SUMMARY — Mean Accuracy across Folds {title_suffix}\")\n    print(f\"{'Dataset':<25} {'FIGS':>8} {'RO-FIGS':>8} {'SG-FIGS':>8} {'GB':>8}\")\n    print(\"-\" * 70)\n    for ds_name in dataset_order:\n        row = f\"{ds_name:<25}\"\n        for method in methods:\n            matches = [\n                s for s in summary\n                if s[\"dataset\"] == ds_name and s[\"method\"] == method\n            ]\n            if matches:\n                row += f\" {matches[0]['accuracy_mean']:>7.4f}\"\n            else:\n                row += f\" {'N/A':>7}\"\n        print(row)\n    print(\"=\" * 70)\n\n    # ---- Print interpretability scores ----\n    print(f\"\\n{'Dataset':<25} {'FIGS':>8} {'RO-FIGS':>8} {'SG-FIGS':>8}\")\n    print(\"-\" * 55)\n    for ds_name in dataset_order:\n        row = f\"{ds_name:<25}\"\n        for method in [\"FIGS\", \"RO-FIGS\", \"SG-FIGS\"]:\n            matches = [\n                s for s in summary\n                if s[\"dataset\"] == ds_name and s[\"method\"] == method\n            ]\n            if matches and matches[0].get(\"interpretability_mean\") is not None:\n                row += f\" {matches[0]['interpretability_mean']:>7.4f}\"\n            else:\n                row += f\" {'N/A':>7}\"\n        print(row)\n    print(\"(SG-FIGS achieves 1.0 interpretability by construction)\")\n\n    # ---- Bar chart: Accuracy by method ----\n    fig, ax = plt.subplots(figsize=(max(8, len(dataset_order) * 1.2), 5))\n\n    x = np.arange(len(dataset_order))\n    width = 0.2\n    colors = [\"#4e79a7\", \"#f28e2b\", \"#e15759\", \"#76b7b2\"]\n\n    for i, method in enumerate(methods):\n        accs = []\n        stds = []\n        for ds_name in dataset_order:\n            matches = [\n                s for s in summary\n                if s[\"dataset\"] == ds_name and s[\"method\"] == method\n            ]\n            if matches:\n                accs.append(matches[0][\"accuracy_mean\"])\n                stds.append(matches[0][\"accuracy_std\"])\n            else:\n                accs.append(0)\n                stds.append(0)\n\n        ax.bar(\n            x + i * width, accs, width,\n            label=method, color=colors[i], alpha=0.85,\n            yerr=stds, capsize=3,\n        )\n\n    ax.set_xlabel(\"Dataset\")\n    ax.set_ylabel(\"Mean Accuracy\")\n    ax.set_title(f\"SG-FIGS vs Baselines: Accuracy Comparison {title_suffix}\")\n    ax.set_xticks(x + width * 1.5)\n    ax.set_xticklabels(dataset_order, rotation=45, ha=\"right\")\n    ax.legend(loc=\"lower right\")\n    ax.set_ylim(0, 1.15)\n    ax.grid(axis=\"y\", alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\nvisualize_results(summary_mini, dataset_order_mini, METHODS, title_suffix=\"(Quick Demo)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "747j5fyexkb",
   "source": "## Part 2 — (Almost) Full Run\n\nLoad the full dataset (12 datasets, 9076 examples) and run with near-original parameters.\nReduced slightly to fit within notebook runtime limits.\n- `MAX_RULES_CANDIDATES = [5, 10]` (original: `[5, 10, 15]`)\n- `gb_n_estimators = 50` (original: `100`)\n- 5-fold CV on all 12 datasets",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "p4ab879shzn",
   "source": "data = load_full()\nprint(f\"Loaded {len(data['datasets'])} dataset entries:\")\nfor ds in data[\"datasets\"]:\n    print(f\"  {ds['dataset']}: {len(ds['examples'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "pkue0g4jry",
   "source": "# --- (Almost) Full Run: near-original parameters ---\n# MAX_RULES_CANDIDATES_FULL = [5, 10, 15]  # True original values\nMAX_RULES_CANDIDATES_FULL = [5, 10]  # Reduced for notebook runtime\nMAX_EXAMPLES_FULL = None  # No limit\n\ndatasets_full = load_all_datasets(data, max_examples=MAX_EXAMPLES_FULL)\nprint(f\"\\nLoaded {len(datasets_full)} datasets total\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7ddv01tj1h3",
   "source": "summary_full, fold_metrics_full, dataset_order_full = run_experiment(\n    datasets=datasets_full,\n    max_rules_candidates=MAX_RULES_CANDIDATES_FULL,  # [5, 10] — near-original\n    methods=METHODS,\n    # gb_n_estimators=100,  # True original value\n    gb_n_estimators=50,  # Reduced for notebook runtime\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7nu4595d3on",
   "source": "visualize_results(summary_full, dataset_order_full, METHODS, title_suffix=\"((Almost) Full Run)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}