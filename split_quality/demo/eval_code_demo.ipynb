{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pm84jac8ps",
   "source": "# Information-Theoretic Split Quality Audit for SG-FIGS\n\nThis notebook evaluates whether **SG-FIGS oblique splits** capture more target information than random feature subsets of the same size, directly testing the PID synergy premise.\n\n**Six metrics are computed per oblique split:**\n1. **Synergy Concentration Ratio (SCR)** — mean split synergy / mean all synergy (>1 = concentrates high-synergy pairs)\n2. **Above-Median Fraction** — fraction of split feature pairs with above-median synergy (random baseline = 0.5)\n3. **Impurity Reduction per Feature** — oblique impurity per feature vs best axis-aligned split\n4. **Redundancy Load** — mean redundancy among split features vs expected random\n5. **Joint MI Coverage** — total joint MI of split pairs vs sum of individual MI\n6. **Synergy-Weighted Efficiency** — (impurity × mean_synergy) / best_axis_impurity\n\n**Part 1** runs a quick demo on a curated subset (3 datasets). **Part 2** runs on all 9 dataset×threshold combinations (144 examples).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "oidtprcykzb",
   "source": "import json\nimport math\nfrom itertools import combinations\nfrom typing import Any\n\nimport matplotlib.pyplot as plt\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gq57rj6aqpw",
   "source": "GITHUB_FULL_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-ac2586-synergy-guided-oblique-splits-using-part/main/split_quality/demo/full_demo_data.json\"\nGITHUB_MINI_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-ac2586-synergy-guided-oblique-splits-using-part/main/split_quality/demo/mini_demo_data.json\"\n\nimport json, os\n\ndef _load_json(url, local_path):\n    try:\n        import urllib.request\n        with urllib.request.urlopen(url) as response:\n            return json.loads(response.read().decode())\n    except Exception: pass\n    if os.path.exists(local_path):\n        with open(local_path) as f: return json.load(f)\n    raise FileNotFoundError(f\"Could not load {local_path}\")\n\ndef load_mini():\n    return _load_json(GITHUB_MINI_DATA_URL, \"mini_demo_data.json\")\n\ndef load_full():\n    return _load_json(GITHUB_FULL_DATA_URL, \"full_demo_data.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "i02vziiagh",
   "source": "## Part 1 — Quick Demo (Mini Data)\n\nLoad a curated subset of 3 dataset×threshold combinations (diabetes SG-FIGS-10, heart_statlog SG-FIGS-25, breast_cancer SG-FIGS-50) to demonstrate the audit pipeline quickly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "uqam3owv5ea",
   "source": "data = load_mini()\nprint(f\"Loaded {len(data['datasets'])} datasets, \"\n      f\"{sum(len(d['examples']) for d in data['datasets'])} total examples\")\nprint(f\"Datasets: {[d['dataset'] for d in data['datasets']]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "i5w65jn88k9",
   "source": "### Metric Extraction and Aggregation\n\nExtract per-split quality metrics and per-fold performance context from each dataset block. The data contains two types of examples:\n- **Split quality** examples (with `eval_scr`, `eval_above_median_fraction`, etc.)\n- **Performance context** examples (with `eval_accuracy`, `eval_auc`)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "aa7swk4dndv",
   "source": "# ---------------------------------------------------------------------------\n# Metric aggregation helpers (from eval.py)\n# ---------------------------------------------------------------------------\ndef safe_mean(vals: list[float]) -> float:\n    return sum(vals) / len(vals) if vals else 0.0\n\ndef safe_std(vals: list[float]) -> float:\n    if len(vals) < 2:\n        return 0.0\n    m = safe_mean(vals)\n    return math.sqrt(sum((v - m) ** 2 for v in vals) / (len(vals) - 1))\n\n\ndef extract_and_aggregate(data: dict) -> dict:\n    \"\"\"Extract per-split metrics and aggregate across all datasets.\n    \n    Separates split-quality examples (containing eval_scr) from\n    performance-context examples (containing eval_accuracy).\n    Returns a dict with split_records, perf_records, and metrics_agg.\n    \"\"\"\n    all_scr_values: list[float] = []\n    all_above_median_values: list[float] = []\n    all_impurity_ratios: list[float] = []\n    all_redundancy_ratios: list[float] = []\n    all_joint_mi_ratios: list[float] = []\n    all_synergy_efficiency: list[float] = []\n\n    split_records: list[dict] = []\n    perf_records: list[dict] = []\n\n    for ds_block in data[\"datasets\"]:\n        ds_name = ds_block[\"dataset\"]\n\n        for ex in ds_block[\"examples\"]:\n            inp = json.loads(ex[\"input\"])\n\n            # Performance context examples\n            if inp.get(\"type\") == \"performance_context\":\n                perf_records.append({\n                    \"dataset\": ds_name,\n                    \"method\": inp.get(\"method\", ex.get(\"metadata_method\", \"\")),\n                    \"fold\": inp.get(\"fold\", 0),\n                    \"accuracy\": ex.get(\"eval_accuracy\", 0.0),\n                    \"auc\": ex.get(\"eval_auc\", 0.0),\n                })\n                continue\n\n            # Split quality examples\n            scr = ex.get(\"eval_scr\", 0.0)\n            above_med = ex.get(\"eval_above_median_fraction\", 0.0)\n            imp_ratio = ex.get(\"eval_impurity_per_feature_ratio\", 0.0)\n            red_ratio = ex.get(\"eval_redundancy_ratio\", 0.0)\n            jmi_ratio = ex.get(\"eval_joint_mi_ratio\", 0.0)\n            syn_eff = ex.get(\"eval_synergy_weighted_efficiency\", 0.0)\n\n            if scr != 0.0:\n                all_scr_values.append(scr)\n            if above_med != 0.0:\n                all_above_median_values.append(above_med)\n            if imp_ratio != 0.0:\n                all_impurity_ratios.append(imp_ratio)\n            if red_ratio != 0.0:\n                all_redundancy_ratios.append(red_ratio)\n            if jmi_ratio != 0.0:\n                all_joint_mi_ratios.append(jmi_ratio)\n            if syn_eff != 0.0:\n                all_synergy_efficiency.append(syn_eff)\n\n            split_records.append({\n                \"dataset\": ds_name,\n                \"method\": ex.get(\"metadata_method\", \"\"),\n                \"features\": inp.get(\"features\", []),\n                \"n_features\": inp.get(\"n_features\", 0),\n                \"rule_str\": ex.get(\"metadata_rule_str\", \"\"),\n                \"scr\": scr,\n                \"above_median_fraction\": above_med,\n                \"impurity_per_feature_ratio\": imp_ratio,\n                \"redundancy_ratio\": red_ratio,\n                \"joint_mi_ratio\": jmi_ratio,\n                \"synergy_weighted_efficiency\": syn_eff,\n                \"impurity_reduction\": ex.get(\"metadata_impurity_reduction\", 0.0),\n                \"best_axis_impurity\": ex.get(\"metadata_best_axis_impurity\", 0.0),\n            })\n\n    metrics_agg: dict[str, float] = {\n        \"mean_scr\": round(safe_mean(all_scr_values), 6),\n        \"std_scr\": round(safe_std(all_scr_values), 6),\n        \"n_oblique_splits_evaluated\": len(all_scr_values),\n        \"mean_above_median_fraction\": round(safe_mean(all_above_median_values), 6),\n        \"std_above_median_fraction\": round(safe_std(all_above_median_values), 6),\n        \"expected_random_above_median\": 0.5,\n        \"mean_impurity_per_feature_ratio\": round(safe_mean(all_impurity_ratios), 6),\n        \"std_impurity_per_feature_ratio\": round(safe_std(all_impurity_ratios), 6),\n        \"mean_redundancy_ratio\": round(safe_mean(all_redundancy_ratios), 6),\n        \"std_redundancy_ratio\": round(safe_std(all_redundancy_ratios), 6),\n        \"mean_joint_mi_ratio\": round(safe_mean(all_joint_mi_ratios), 6),\n        \"std_joint_mi_ratio\": round(safe_std(all_joint_mi_ratios), 6),\n        \"mean_synergy_weighted_efficiency\": round(safe_mean(all_synergy_efficiency), 6),\n        \"std_synergy_weighted_efficiency\": round(safe_std(all_synergy_efficiency), 6),\n        \"n_datasets_with_oblique_splits\": len(\n            set(r[\"dataset\"].rsplit(\"_SG-FIGS-\", 1)[0] for r in split_records)\n        ),\n        \"total_examples\": sum(len(d[\"examples\"]) for d in data[\"datasets\"]),\n    }\n\n    return {\n        \"split_records\": split_records,\n        \"perf_records\": perf_records,\n        \"metrics_agg\": metrics_agg,\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bzdjot8me3e",
   "source": "# Run extraction on mini data\nresults = extract_and_aggregate(data)\nsplit_records = results[\"split_records\"]\nperf_records = results[\"perf_records\"]\nmetrics_agg = results[\"metrics_agg\"]\n\nprint(f\"Split quality records: {len(split_records)}\")\nprint(f\"Performance records:   {len(perf_records)}\")\nprint(f\"\\n--- Per-Split Details ---\")\nfor rec in split_records:\n    print(f\"  {rec['dataset']:30s}  SCR={rec['scr']:.3f}  \"\n          f\"AboveMed={rec['above_median_fraction']:.3f}  \"\n          f\"ImpRatio={rec['impurity_per_feature_ratio']:.3f}  \"\n          f\"RedRatio={rec['redundancy_ratio']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "l61q9grfd9",
   "source": "### Aggregate Results and Interpretation\n\nPrint the aggregate metrics and interpret whether SG-FIGS oblique splits effectively concentrate synergistic feature pairs (from eval.py lines 531–611).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2rqtdgf0yf7",
   "source": "# ---------------------------------------------------------------------------\n# Aggregate results and interpretation (from eval.py main())\n# ---------------------------------------------------------------------------\nprint(\"=\" * 60)\nprint(\"AGGREGATE RESULTS\")\nprint(\"=\" * 60)\nfor k, v in metrics_agg.items():\n    print(f\"  {k}: {v}\")\n\nprint()\nprint(\"=\" * 60)\nprint(\"INTERPRETATION SUMMARY\")\nprint(\"=\" * 60)\n\nscr_mean = metrics_agg[\"mean_scr\"]\nprint(\n    f\"SCR = {scr_mean:.3f} \"\n    f\"({'> 1: splits concentrate high-synergy pairs' if scr_mean > 1 else '<= 1: splits do NOT preferentially select high-synergy pairs'})\"\n)\n\nabove_med = metrics_agg[\"mean_above_median_fraction\"]\nprint(\n    f\"Above-median fraction = {above_med:.3f} (random baseline = 0.5, \"\n    f\"{'better' if above_med > 0.5 else 'worse'} than random)\"\n)\n\nimp_ratio = metrics_agg[\"mean_impurity_per_feature_ratio\"]\nprint(\n    f\"Impurity/feature ratio = {imp_ratio:.3f} \"\n    f\"({'oblique splits MORE efficient per feature' if imp_ratio > 1 else 'oblique splits LESS efficient per feature than best axis-aligned'})\"\n)\n\nred_ratio = metrics_agg[\"mean_redundancy_ratio\"]\nprint(\n    f\"Redundancy ratio = {red_ratio:.3f} \"\n    f\"({'higher than random: features overlap' if red_ratio > 1 else 'lower than random: features complementary'})\"\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rb8tpodqp7s",
   "source": "### Visualization\n\nVisualize the six split-quality metrics across all oblique splits and show per-dataset performance context.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "f4y5hearz6w",
   "source": "def visualize_results(split_records, perf_records, metrics_agg, title_prefix=\"\"):\n    \"\"\"Reusable visualization for split quality audit results.\n    \n    Produces:\n    1. Bar chart of the 6 aggregate metrics with reference lines\n    2. Per-split radar-style comparison if enough splits exist\n    3. Performance context summary table\n    \"\"\"\n    # --- Figure 1: Aggregate metric bar chart ---\n    metric_names = [\"SCR\", \"Above-Med\\nFraction\", \"Impurity/\\nFeature Ratio\",\n                     \"Redundancy\\nRatio\", \"Joint MI\\nRatio\", \"Synergy-Wtd\\nEfficiency\"]\n    metric_keys = [\"mean_scr\", \"mean_above_median_fraction\",\n                    \"mean_impurity_per_feature_ratio\", \"mean_redundancy_ratio\",\n                    \"mean_joint_mi_ratio\", \"mean_synergy_weighted_efficiency\"]\n    std_keys = [\"std_scr\", \"std_above_median_fraction\",\n                \"std_impurity_per_feature_ratio\", \"std_redundancy_ratio\",\n                \"std_joint_mi_ratio\", \"std_synergy_weighted_efficiency\"]\n    # Reference lines: SCR>1 good, AboveMed>0.5 baseline, ImpRatio>1 means better, etc.\n    references = [1.0, 0.5, 1.0, 1.0, 1.0, None]\n\n    means = [metrics_agg[k] for k in metric_keys]\n    stds = [metrics_agg[k] for k in std_keys]\n\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Left: Bar chart of aggregate metrics\n    ax = axes[0]\n    x = np.arange(len(metric_names))\n    bars = ax.bar(x, means, yerr=stds, capsize=4, color=\"#4C72B0\", alpha=0.8, edgecolor=\"black\")\n    for i, ref in enumerate(references):\n        if ref is not None:\n            ax.plot([i - 0.4, i + 0.4], [ref, ref], \"r--\", linewidth=1.5, alpha=0.7)\n    ax.set_xticks(x)\n    ax.set_xticklabels(metric_names, fontsize=9)\n    ax.set_ylabel(\"Value\")\n    ax.set_title(f\"{title_prefix}Aggregate Split Quality Metrics\\n\"\n                 f\"(n={metrics_agg['n_oblique_splits_evaluated']} oblique splits, \"\n                 f\"red dashed = reference)\")\n    ax.grid(axis=\"y\", alpha=0.3)\n\n    # Right: Per-split SCR values colored by dataset\n    ax2 = axes[1]\n    if split_records:\n        datasets = sorted(set(r[\"dataset\"] for r in split_records))\n        colors = plt.cm.Set2(np.linspace(0, 1, max(len(datasets), 3)))\n        ds_color = {ds: colors[i] for i, ds in enumerate(datasets)}\n\n        labels = []\n        scr_vals = []\n        bar_colors = []\n        for rec in split_records:\n            short_name = rec[\"dataset\"].replace(\"_SG-FIGS-\", \"\\nSG-FIGS-\")\n            labels.append(short_name)\n            scr_vals.append(rec[\"scr\"])\n            bar_colors.append(ds_color[rec[\"dataset\"]])\n\n        x2 = np.arange(len(labels))\n        ax2.bar(x2, scr_vals, color=bar_colors, alpha=0.8, edgecolor=\"black\")\n        ax2.axhline(y=1.0, color=\"red\", linestyle=\"--\", linewidth=1.5, alpha=0.7, label=\"SCR=1 (random)\")\n        ax2.set_xticks(x2)\n        ax2.set_xticklabels(labels, fontsize=8, rotation=0, ha=\"center\")\n        ax2.set_ylabel(\"Synergy Concentration Ratio\")\n        ax2.set_title(f\"{title_prefix}Per-Split SCR Values\")\n        ax2.legend(fontsize=8)\n        ax2.grid(axis=\"y\", alpha=0.3)\n    else:\n        ax2.text(0.5, 0.5, \"No split records\", ha=\"center\", va=\"center\")\n\n    plt.tight_layout()\n    plt.show()\n\n    # --- Performance context summary ---\n    if perf_records:\n        print(f\"\\n{'='*60}\")\n        print(f\"PERFORMANCE CONTEXT SUMMARY ({len(perf_records)} fold records)\")\n        print(f\"{'='*60}\")\n        # Group by dataset\n        from collections import defaultdict\n        by_ds = defaultdict(list)\n        for rec in perf_records:\n            by_ds[rec[\"dataset\"]].append(rec)\n        for ds_name in sorted(by_ds.keys()):\n            recs = by_ds[ds_name]\n            acc_vals = [r[\"accuracy\"] for r in recs if r[\"accuracy\"] > 0]\n            auc_vals = [r[\"auc\"] for r in recs if r[\"auc\"] > 0]\n            print(f\"  {ds_name}:\")\n            if acc_vals:\n                print(f\"    Accuracy: mean={safe_mean(acc_vals):.4f}, \"\n                      f\"std={safe_std(acc_vals):.4f}, n={len(acc_vals)}\")\n            if auc_vals:\n                print(f\"    AUC:      mean={safe_mean(auc_vals):.4f}, \"\n                      f\"std={safe_std(auc_vals):.4f}, n={len(auc_vals)}\")\n\n\n# Call visualization for Part 1\nvisualize_results(split_records, perf_records, metrics_agg, title_prefix=\"[Mini] \")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "szpm1tvxsbc",
   "source": "## Part 2 — Full Run (Original Parameters)\n\nLoad all 9 dataset×threshold combinations (3 datasets × 3 thresholds = 9 oblique splits + 135 performance fold records = 144 total examples) and re-run the same extraction and visualization pipeline.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "scn7uoh71fb",
   "source": "data = load_full()\nprint(f\"Loaded {len(data['datasets'])} datasets, \"\n      f\"{sum(len(d['examples']) for d in data['datasets'])} total examples\")\nprint(f\"Datasets: {[d['dataset'] for d in data['datasets']]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "z7t906hyyyg",
   "source": "# Run extraction on full data — all 9 dataset×threshold combinations\nresults_full = extract_and_aggregate(data)\nsplit_records_full = results_full[\"split_records\"]\nperf_records_full = results_full[\"perf_records\"]\nmetrics_agg_full = results_full[\"metrics_agg\"]\n\nprint(f\"Split quality records: {len(split_records_full)}\")\nprint(f\"Performance records:   {len(perf_records_full)}\")\nprint(f\"\\n--- Per-Split Details ---\")\nfor rec in split_records_full:\n    print(f\"  {rec['dataset']:30s}  SCR={rec['scr']:.3f}  \"\n          f\"AboveMed={rec['above_median_fraction']:.3f}  \"\n          f\"ImpRatio={rec['impurity_per_feature_ratio']:.3f}  \"\n          f\"RedRatio={rec['redundancy_ratio']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ed228txgyat",
   "source": "# Full aggregate results and interpretation\nprint(\"=\" * 60)\nprint(\"AGGREGATE RESULTS (Full)\")\nprint(\"=\" * 60)\nfor k, v in metrics_agg_full.items():\n    print(f\"  {k}: {v}\")\n\nprint()\nprint(\"=\" * 60)\nprint(\"INTERPRETATION SUMMARY (Full)\")\nprint(\"=\" * 60)\n\nscr_mean = metrics_agg_full[\"mean_scr\"]\nprint(\n    f\"SCR = {scr_mean:.3f} \"\n    f\"({'> 1: splits concentrate high-synergy pairs' if scr_mean > 1 else '<= 1: splits do NOT preferentially select high-synergy pairs'})\"\n)\n\nabove_med = metrics_agg_full[\"mean_above_median_fraction\"]\nprint(\n    f\"Above-median fraction = {above_med:.3f} (random baseline = 0.5, \"\n    f\"{'better' if above_med > 0.5 else 'worse'} than random)\"\n)\n\nimp_ratio = metrics_agg_full[\"mean_impurity_per_feature_ratio\"]\nprint(\n    f\"Impurity/feature ratio = {imp_ratio:.3f} \"\n    f\"({'oblique splits MORE efficient per feature' if imp_ratio > 1 else 'oblique splits LESS efficient per feature than best axis-aligned'})\"\n)\n\nred_ratio = metrics_agg_full[\"mean_redundancy_ratio\"]\nprint(\n    f\"Redundancy ratio = {red_ratio:.3f} \"\n    f\"({'higher than random: features overlap' if red_ratio > 1 else 'lower than random: features complementary'})\"\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "xfa0p833g1e",
   "source": "# Call the same reusable visualization function for full results\nvisualize_results(split_records_full, perf_records_full, metrics_agg_full, title_prefix=\"[Full] \")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}