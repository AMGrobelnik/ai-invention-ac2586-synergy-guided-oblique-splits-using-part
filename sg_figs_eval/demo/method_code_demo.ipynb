{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1okoitmdpk",
   "source": "# SG-FIGS: Fold-Aware Synergy-Guided Oblique Splits with Threshold Ablation\n\nThis notebook demonstrates the SG-FIGS experiment, which compares 6 methods across tabular classification benchmarks:\n\n1. **FIGS** — Standard axis-aligned splits\n2. **RO-FIGS** — Random oblique splits (no synergy guidance)\n3. **SG-FIGS-10** — Synergy threshold p90 (aggressive)\n4. **SG-FIGS-25** — Synergy threshold p75 (moderate)\n5. **SG-FIGS-50** — Synergy threshold p50 (permissive)\n6. **GradientBoosting** — Non-interpretable baseline\n\nThe experiment implements fold-aware PID synergy graphs, oblique splits via Ridge projection, non-circular interpretability scoring, and statistical tests (Wilcoxon, Friedman/Nemenyi).\n\n- **Part 1 (Quick Demo)**: Runs on 3 datasets with 2-fold CV for fast iteration\n- **Part 2 (Full Run)**: Runs on all 12 datasets with 5-fold CV (original parameters)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ph8wxeaupmj",
   "source": "# ── NUMPY MONKEY-PATCH (must be before dit import) ──────────────────────────\nimport numpy as np\nif not hasattr(np, 'alltrue'):\n    np.alltrue = np.all\nif not hasattr(np, 'cumproduct'):\n    np.cumproduct = np.cumprod\nif not hasattr(np, 'product'):\n    np.product = np.prod\n\n# ── IMPORTS ──────────────────────────────────────────────────────────────────\nimport json\nimport sys\nimport time\nimport warnings\nfrom pathlib import Path\n\nimport dit\nfrom dit.pid import PID_WB\nfrom imodels import FIGSClassifier\nfrom imodels.tree.figs import Node\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import (\n    KBinsDiscretizer, StandardScaler, OrdinalEncoder, LabelEncoder\n)\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.datasets import load_breast_cancer, load_wine, fetch_openml\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom scipy.stats import wilcoxon, friedmanchisquare, rankdata, ttest_rel\nfrom scipy.stats import studentized_range\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams['figure.dpi'] = 100\n\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "z74czd08csg",
   "source": "# ── DATA LOADING HELPERS ─────────────────────────────────────────────────────\nGITHUB_FULL_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-ac2586-synergy-guided-oblique-splits-using-part/main/sg_figs_eval/demo/full_demo_data.json\"\nGITHUB_MINI_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-ac2586-synergy-guided-oblique-splits-using-part/main/sg_figs_eval/demo/mini_demo_data.json\"\nimport json, os\n\ndef _load_json(url, local_path):\n    try:\n        import urllib.request\n        with urllib.request.urlopen(url) as response:\n            return json.loads(response.read().decode())\n    except Exception: pass\n    if os.path.exists(local_path):\n        with open(local_path) as f: return json.load(f)\n    raise FileNotFoundError(f\"Could not load {local_path}\")\n\ndef load_mini():\n    return _load_json(GITHUB_MINI_DATA_URL, \"mini_demo_data.json\")\n\ndef load_full():\n    return _load_json(GITHUB_FULL_DATA_URL, \"full_demo_data.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d5rcbsingpk",
   "source": "---\n## Part 1 — Quick Demo (Mini Data)\n\nRuns on 3 datasets (banknote, breast_cancer, wine) with **2-fold CV** and a single `max_rules=5` setting for fast execution.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4ig9bqk3rv",
   "source": "data = load_mini()\nprint(f\"Loaded {len(data['datasets'])} datasets\")\nfor ds in data['datasets']:\n    print(f\"  {ds['dataset']}: {len(ds['examples'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4wyt6980gpt",
   "source": "### Constants and Dataset Configuration\n\nDefines the 12 benchmark datasets and experiment parameters. For Part 1, we reduce to 3 datasets and 2-fold CV.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "eu32hwudt3",
   "source": "# ── CONSTANTS ────────────────────────────────────────────────────────────────\nDATASETS = {\n    'breast_cancer': {'source': 'sklearn', 'domain': 'medical'},\n    'wine': {'source': 'sklearn', 'domain': 'chemistry'},\n    'diabetes': {'source': 'openml', 'openml_id': 37, 'domain': 'medical'},\n    'heart_statlog': {'source': 'openml', 'openml_id': 53, 'domain': 'medical'},\n    'ionosphere': {'source': 'openml', 'openml_id': 59, 'domain': 'physics'},\n    'sonar': {'source': 'openml', 'openml_id': 40, 'domain': 'signal'},\n    'vehicle': {'source': 'openml', 'openml_id': 54, 'domain': 'vision'},\n    'segment': {'source': 'openml', 'openml_id': 36, 'domain': 'vision'},\n    'glass': {'source': 'openml', 'openml_id': 41, 'domain': 'forensics'},\n    'banknote': {'source': 'openml', 'openml_id': 1462, 'domain': 'image'},\n    'credit_g': {'source': 'openml', 'openml_id': 31, 'domain': 'finance'},\n    'australian': {'source': 'openml', 'openml_id': 40981, 'domain': 'finance'},\n}\n\nTHRESHOLDS = {'SG-FIGS-10': 90, 'SG-FIGS-25': 75, 'SG-FIGS-50': 50}\n\n# ── QUICK DEMO PARAMS (reduced for Part 1) ──────────────────────────────────\nDATASET_ORDER = ['banknote', 'wine', 'breast_cancer']  # Original: all 12\nMAX_RULES_VALUES = [5]  # Original: [5, 10, 15]\nN_FOLDS = 2  # Original: 5\nPER_DATASET_TIMEOUT = 120  # Original: 600",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "r0du8myqu0o",
   "source": "### Phase 1 — Dataset Loading\n\nLoads datasets from sklearn or OpenML, handles categorical encoding, and standardizes features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zphn7wex6o9",
   "source": "def load_dataset(name: str, info: dict) -> tuple:\n    \"\"\"Load and preprocess dataset. Returns X (float64), y (int), feature_names.\"\"\"\n    if info['source'] == 'sklearn':\n        loader = load_breast_cancer if name == 'breast_cancer' else load_wine\n        data = loader()\n        X, y, feat_names = data.data, data.target, list(data.feature_names)\n    else:\n        d = fetch_openml(data_id=info['openml_id'], as_frame=True, parser='auto')\n        df = d.frame\n        target_col = d.target.name\n        feat_names = [c for c in df.columns if c != target_col]\n        X_df = df[feat_names]\n        y_raw = df[target_col]\n\n        le = LabelEncoder()\n        y = le.fit_transform(y_raw)\n\n        cat_cols = X_df.select_dtypes(include=['category', 'object']).columns.tolist()\n\n        X = np.zeros((len(X_df), len(feat_names)), dtype=float)\n        for i, col in enumerate(feat_names):\n            if col in cat_cols:\n                oe = OrdinalEncoder(\n                    handle_unknown='use_encoded_value',\n                    unknown_value=-1,\n                )\n                X[:, i] = oe.fit_transform(X_df[[col]]).ravel()\n            else:\n                vals = X_df[col].values\n                # Handle potential NaN\n                vals = np.where(np.isnan(vals.astype(float)), 0.0, vals.astype(float))\n                X[:, i] = vals\n\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    y = y.astype(int)\n    return X, y, feat_names",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "moaog90lafj",
   "source": "### Phase 2 — Synergy Graph Construction (Fold-Aware)\n\nComputes pairwise PID synergy between features using only training data, then builds an adjacency graph by thresholding.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bmm2ytxv85w",
   "source": "def _compute_single_synergy(\n    x1: np.ndarray,\n    x2: np.ndarray,\n    y: np.ndarray,\n) -> float:\n    \"\"\"Compute PID synergy between two discretized features and target.\"\"\"\n    probs: dict = {}\n    for idx in range(len(y)):\n        key = (int(x1[idx]), int(x2[idx]), int(y[idx]))\n        probs[key] = probs.get(key, 0) + 1\n    total = sum(probs.values())\n    if total == 0:\n        return 0.0\n    outcomes = list(probs.keys())\n    pmf = [probs[o] / total for o in outcomes]\n\n    d = dit.Distribution(outcomes, pmf)\n    pid = PID_WB(d, [[0], [1]], [2])\n    syn = pid.get_pi(pid._lattice.top)\n    return float(syn)\n\n\ndef compute_pairwise_synergy_matrix(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    n_bins: int = 5,\n    max_features: int = 30,\n) -> tuple:\n    \"\"\"Compute pairwise PID synergy for all feature pairs using ONLY training data.\"\"\"\n    t0 = time.time()\n    p = X_train.shape[1]\n\n    # Pre-filter features for high-dimensional datasets\n    feature_mask = np.arange(p)\n    if p > max_features:\n        from sklearn.feature_selection import mutual_info_classif\n        mi_scores = mutual_info_classif(\n            X_train, y_train, random_state=42, n_neighbors=5,\n        )\n        top_idx = np.argsort(mi_scores)[-max_features:]\n        feature_mask = np.sort(top_idx)\n\n    # Discretize on training data only\n    kbd = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n    try:\n        X_disc = kbd.fit_transform(X_train).astype(int)\n    except ValueError:\n        kbd = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n        X_disc = kbd.fit_transform(X_train).astype(int)\n    y_disc = y_train.astype(int)\n\n    synergy_matrix = np.zeros((p, p))\n    for ii in range(len(feature_mask)):\n        i = feature_mask[ii]\n        for jj in range(ii + 1, len(feature_mask)):\n            j = feature_mask[jj]\n            try:\n                syn = _compute_single_synergy(X_disc[:, i], X_disc[:, j], y_disc)\n            except Exception:\n                syn = 0.0\n            synergy_matrix[i, j] = syn\n            synergy_matrix[j, i] = syn\n\n    elapsed = time.time() - t0\n    return synergy_matrix, elapsed\n\n\ndef compute_fast_synergy_proxy(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    n_bins: int = 5,\n) -> np.ndarray:\n    \"\"\"Fast MI-based synergy proxy for validation set.\n\n    Synergy ≈ I(X_i, X_j; Y) - I(X_i; Y) - I(X_j; Y)\n    \"\"\"\n    from sklearn.metrics import mutual_info_score\n    p = X_train.shape[1]\n\n    kbd = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n    try:\n        X_disc = kbd.fit_transform(X_train).astype(int)\n    except ValueError:\n        kbd = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n        X_disc = kbd.fit_transform(X_train).astype(int)\n    y_disc = y_train.astype(int)\n\n    mi_individual = np.zeros(p)\n    for i in range(p):\n        mi_individual[i] = mutual_info_score(X_disc[:, i], y_disc)\n\n    synergy_proxy = np.zeros((p, p))\n    for i in range(p):\n        for j in range(i + 1, p):\n            joint = X_disc[:, i] * (n_bins + 1) + X_disc[:, j]\n            mi_joint = mutual_info_score(joint.astype(int), y_disc)\n            interaction = mi_joint - mi_individual[i] - mi_individual[j]\n            synergy_proxy[i, j] = max(interaction, 0.0)\n            synergy_proxy[j, i] = synergy_proxy[i, j]\n\n    return synergy_proxy\n\n\ndef build_synergy_graph(\n    synergy_matrix: np.ndarray,\n    threshold_percentile: float,\n) -> tuple:\n    \"\"\"Build adjacency from synergy matrix by keeping edges above the given percentile.\"\"\"\n    p = synergy_matrix.shape[0]\n    upper_tri = synergy_matrix[np.triu_indices(p, k=1)]\n\n    if len(upper_tri) == 0 or np.all(upper_tri == 0):\n        return {i: set() for i in range(p)}, 0, 0.0\n\n    cutoff = float(np.percentile(upper_tri, threshold_percentile))\n    MIN_SYNERGY = 1e-6\n\n    adj: dict = {i: set() for i in range(p)}\n    n_edges = 0\n    for i in range(p):\n        for j in range(i + 1, p):\n            if synergy_matrix[i, j] >= cutoff and synergy_matrix[i, j] > MIN_SYNERGY:\n                adj[i].add(j)\n                adj[j].add(i)\n                n_edges += 1\n\n    return adj, n_edges, cutoff",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kp6n2w0x5mr",
   "source": "### Phase 3 — SG-FIGS and RO-FIGS Implementation\n\nCustom FIGS classifiers with synergy-guided oblique splits (SG-FIGS) and random oblique splits (RO-FIGS). Both extend `FIGSClassifier` from the `imodels` library, adding oblique split capability via Ridge regression projections.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pvnychckjvm",
   "source": "class SynergyGuidedFIGS(FIGSClassifier):\n    \"\"\"FIGS with synergy-guided oblique splits.\"\"\"\n\n    def __init__(\n        self,\n        synergy_adj: dict,\n        synergy_matrix: np.ndarray,\n        max_rules: int = 12,\n        ridge_alpha: float = 1.0,\n        max_features_per_split: int = 5,\n        random_state: int = None,\n    ):\n        super().__init__(max_rules=max_rules, random_state=random_state)\n        self.synergy_adj = synergy_adj\n        self.synergy_matrix = synergy_matrix\n        self.ridge_alpha = ridge_alpha\n        self.max_features_per_split = max_features_per_split\n        self.oblique_splits_info: list = []\n\n    def _construct_node_with_stump(\n        self, X, y, idxs, tree_num,\n        sample_weight=None, compare_nodes_with_sample_weight=True,\n        max_features=None, depth=None,\n    ):\n        \"\"\"Override: try synergy-guided oblique splits alongside axis-aligned.\"\"\"\n        node_axis = super()._construct_node_with_stump(\n            X, y, idxs, tree_num,\n            sample_weight=sample_weight,\n            compare_nodes_with_sample_weight=compare_nodes_with_sample_weight,\n            max_features=max_features,\n            depth=depth,\n        )\n\n        if not hasattr(node_axis, 'left_temp') or node_axis.left_temp is None:\n            node_axis.is_oblique = False\n            return node_axis\n\n        best_node = node_axis\n        best_impurity_reduction = node_axis.impurity_reduction or 0.0\n\n        best_feature = node_axis.feature\n        if best_feature is not None and best_feature in self.synergy_adj:\n            neighbors = self.synergy_adj[best_feature]\n            if len(neighbors) > 0:\n                feat_subset = sorted([best_feature] + list(neighbors))\n\n                if len(feat_subset) > self.max_features_per_split:\n                    syn_scores = [\n                        (f, self.synergy_matrix[best_feature, f])\n                        for f in neighbors\n                    ]\n                    syn_scores.sort(key=lambda x: x[1], reverse=True)\n                    feat_subset = [best_feature] + [\n                        f for f, _ in syn_scores[:self.max_features_per_split - 1]\n                    ]\n\n                if len(feat_subset) >= 2:\n                    oblique_node = self._try_oblique_split(\n                        X, y, idxs, tree_num, feat_subset,\n                        sample_weight=sample_weight,\n                        depth=depth,\n                    )\n                    if oblique_node is not None:\n                        obl_imp = oblique_node.impurity_reduction or 0.0\n                        if obl_imp > best_impurity_reduction:\n                            best_node = oblique_node\n                            best_impurity_reduction = obl_imp\n\n        if not hasattr(best_node, 'is_oblique'):\n            best_node.is_oblique = False\n        return best_node\n\n    def _try_oblique_split(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        idxs: np.ndarray,\n        tree_num: int,\n        feat_subset: list,\n        sample_weight: np.ndarray = None,\n        depth: int = None,\n    ):\n        \"\"\"Try an oblique split using Ridge on the feature subset.\"\"\"\n        try:\n            X_sub = X[idxs][:, feat_subset]\n            y_sub = y[idxs]\n            if y_sub.ndim > 1:\n                y_fit = y_sub[:, 0]\n            else:\n                y_fit = y_sub.copy().astype(float)\n\n            if len(np.unique(y_fit)) < 2:\n                return None\n\n            ridge = Ridge(alpha=self.ridge_alpha)\n            ridge.fit(X_sub, y_fit)\n            weights = ridge.coef_\n            intercept = float(ridge.intercept_)\n            projection = X_sub @ weights + intercept\n\n            dt = DecisionTreeRegressor(max_depth=1)\n            sweight = None\n            if sample_weight is not None:\n                sweight = sample_weight[idxs]\n            dt.fit(projection.reshape(-1, 1), y_sub, sample_weight=sweight)\n\n            if dt.tree_.feature[0] == -2 or len(dt.tree_.feature) < 3:\n                return None\n\n            threshold = float(dt.tree_.threshold[0])\n            impurity = dt.tree_.impurity\n            n_samples = dt.tree_.n_node_samples\n\n            if sample_weight is not None:\n                proj_full = X[idxs][:, feat_subset] @ weights + intercept\n                idxs_left_mask = proj_full <= threshold\n                n_left = sample_weight[idxs][idxs_left_mask].sum()\n                n_right = sample_weight[idxs][~idxs_left_mask].sum()\n                n_total = n_left + n_right\n            else:\n                n_left = n_samples[1]\n                n_right = n_samples[2]\n                n_total = n_samples[0]\n\n            if n_total == 0:\n                return None\n\n            imp_red = (\n                impurity[0]\n                - impurity[1] * n_left / n_total\n                - impurity[2] * n_right / n_total\n            ) * n_total\n\n            if imp_red <= 0:\n                return None\n\n            proj_full = X[idxs][:, feat_subset] @ weights + intercept\n            idxs_left = idxs.copy()\n            idxs_right = idxs.copy()\n            idxs_left[idxs] = proj_full <= threshold\n            idxs_right[idxs] = proj_full > threshold\n\n            node_oblique = Node(\n                idxs=idxs,\n                value=dt.tree_.value[0],\n                tree_num=tree_num,\n                feature=feat_subset[0],\n                threshold=threshold,\n                impurity=float(impurity[0]),\n                impurity_reduction=float(imp_red),\n                depth=depth,\n            )\n            node_oblique.is_oblique = True\n            node_oblique.oblique_features = feat_subset\n            node_oblique.oblique_weights = weights.tolist()\n            node_oblique.oblique_bias = intercept\n\n            node_oblique.setattrs(\n                left_temp=Node(\n                    idxs=idxs_left,\n                    value=dt.tree_.value[1],\n                    tree_num=tree_num,\n                    depth=(depth or 0) + 1,\n                ),\n                right_temp=Node(\n                    idxs=idxs_right,\n                    value=dt.tree_.value[2],\n                    tree_num=tree_num,\n                    depth=(depth or 0) + 1,\n                ),\n            )\n\n            self.oblique_splits_info.append({\n                'features': feat_subset,\n                'weights': weights.tolist(),\n                'threshold': float(threshold),\n                'impurity_reduction': float(imp_red),\n            })\n\n            return node_oblique\n\n        except Exception:\n            return None\n\n    def _predict_tree(self, root: Node, X: np.ndarray) -> np.ndarray:\n        \"\"\"Override to handle oblique splits during prediction.\"\"\"\n        def _predict_single(node, x):\n            if node.left is None and node.right is None:\n                return node.value\n\n            if getattr(node, 'is_oblique', False):\n                proj = sum(\n                    w * x[f]\n                    for w, f in zip(node.oblique_weights, node.oblique_features)\n                )\n                proj += node.oblique_bias\n                go_left = proj <= node.threshold\n            else:\n                go_left = x[node.feature] <= node.threshold\n\n            if go_left:\n                return _predict_single(node.left, x) if node.left else node.value\n            else:\n                return _predict_single(node.right, x) if node.right else node.value\n\n        preds = np.zeros((X.shape[0], self.n_outputs))\n        for i in range(X.shape[0]):\n            preds[i] = _predict_single(root, X[i])\n        return preds\n\n\nclass RandomObliqueFIGS(FIGSClassifier):\n    \"\"\"Baseline: oblique splits with RANDOM feature subsets (no synergy guidance).\"\"\"\n\n    def __init__(\n        self,\n        beam_size: int = 3,\n        max_rules: int = 12,\n        ridge_alpha: float = 1.0,\n        random_state: int = None,\n    ):\n        super().__init__(max_rules=max_rules, random_state=random_state)\n        self.beam_size = beam_size\n        self.ridge_alpha = ridge_alpha\n        self.oblique_splits_info: list = []\n        self._rng = np.random.RandomState(random_state)\n\n    def _construct_node_with_stump(\n        self, X, y, idxs, tree_num,\n        sample_weight=None, compare_nodes_with_sample_weight=True,\n        max_features=None, depth=None,\n    ):\n        node_axis = super()._construct_node_with_stump(\n            X, y, idxs, tree_num,\n            sample_weight=sample_weight,\n            compare_nodes_with_sample_weight=compare_nodes_with_sample_weight,\n            max_features=max_features,\n            depth=depth,\n        )\n\n        if not hasattr(node_axis, 'left_temp') or node_axis.left_temp is None:\n            node_axis.is_oblique = False\n            return node_axis\n\n        best_node = node_axis\n        best_imp_red = node_axis.impurity_reduction or 0.0\n\n        p = X.shape[1]\n        for _ in range(3):\n            feat_size = min(self.beam_size, p)\n            if feat_size < 2:\n                continue\n            feat_subset = sorted(\n                self._rng.choice(p, size=feat_size, replace=False).tolist()\n            )\n\n            oblique_node = self._try_oblique_split(\n                X, y, idxs, tree_num, feat_subset,\n                sample_weight=sample_weight,\n                depth=depth,\n            )\n            if oblique_node is not None:\n                obl_imp = oblique_node.impurity_reduction or 0.0\n                if obl_imp > best_imp_red:\n                    best_node = oblique_node\n                    best_imp_red = obl_imp\n\n        if not hasattr(best_node, 'is_oblique'):\n            best_node.is_oblique = False\n        return best_node\n\n    def _try_oblique_split(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        idxs: np.ndarray,\n        tree_num: int,\n        feat_subset: list,\n        sample_weight: np.ndarray = None,\n        depth: int = None,\n    ):\n        \"\"\"Try an oblique split with random feature subset.\"\"\"\n        try:\n            X_sub = X[idxs][:, feat_subset]\n            y_sub = y[idxs]\n            if y_sub.ndim > 1:\n                y_fit = y_sub[:, 0]\n            else:\n                y_fit = y_sub.copy().astype(float)\n\n            if len(np.unique(y_fit)) < 2:\n                return None\n\n            ridge = Ridge(alpha=self.ridge_alpha)\n            ridge.fit(X_sub, y_fit)\n            weights = ridge.coef_\n            intercept = float(ridge.intercept_)\n            projection = X_sub @ weights + intercept\n\n            dt = DecisionTreeRegressor(max_depth=1)\n            sweight = None\n            if sample_weight is not None:\n                sweight = sample_weight[idxs]\n            dt.fit(projection.reshape(-1, 1), y_sub, sample_weight=sweight)\n\n            if dt.tree_.feature[0] == -2 or len(dt.tree_.feature) < 3:\n                return None\n\n            threshold = float(dt.tree_.threshold[0])\n            impurity = dt.tree_.impurity\n            n_samples = dt.tree_.n_node_samples\n\n            if sample_weight is not None:\n                proj_full = X[idxs][:, feat_subset] @ weights + intercept\n                idxs_left_mask = proj_full <= threshold\n                n_left = sample_weight[idxs][idxs_left_mask].sum()\n                n_right = sample_weight[idxs][~idxs_left_mask].sum()\n                n_total = n_left + n_right\n            else:\n                n_left = n_samples[1]\n                n_right = n_samples[2]\n                n_total = n_samples[0]\n\n            if n_total == 0:\n                return None\n\n            imp_red = (\n                impurity[0]\n                - impurity[1] * n_left / n_total\n                - impurity[2] * n_right / n_total\n            ) * n_total\n\n            if imp_red <= 0:\n                return None\n\n            proj_full = X[idxs][:, feat_subset] @ weights + intercept\n            idxs_left = idxs.copy()\n            idxs_right = idxs.copy()\n            idxs_left[idxs] = proj_full <= threshold\n            idxs_right[idxs] = proj_full > threshold\n\n            node_oblique = Node(\n                idxs=idxs,\n                value=dt.tree_.value[0],\n                tree_num=tree_num,\n                feature=feat_subset[0],\n                threshold=threshold,\n                impurity=float(impurity[0]),\n                impurity_reduction=float(imp_red),\n                depth=depth,\n            )\n            node_oblique.is_oblique = True\n            node_oblique.oblique_features = feat_subset\n            node_oblique.oblique_weights = weights.tolist()\n            node_oblique.oblique_bias = intercept\n\n            node_oblique.setattrs(\n                left_temp=Node(\n                    idxs=idxs_left,\n                    value=dt.tree_.value[1],\n                    tree_num=tree_num,\n                    depth=(depth or 0) + 1,\n                ),\n                right_temp=Node(\n                    idxs=idxs_right,\n                    value=dt.tree_.value[2],\n                    tree_num=tree_num,\n                    depth=(depth or 0) + 1,\n                ),\n            )\n\n            self.oblique_splits_info.append({\n                'features': feat_subset,\n                'weights': weights.tolist(),\n                'threshold': float(threshold),\n                'impurity_reduction': float(imp_red),\n            })\n\n            return node_oblique\n\n        except Exception:\n            return None\n\n    def _predict_tree(self, root: Node, X: np.ndarray) -> np.ndarray:\n        \"\"\"Override to handle oblique splits during prediction.\"\"\"\n        def _predict_single(node, x):\n            if node.left is None and node.right is None:\n                return node.value\n\n            if getattr(node, 'is_oblique', False):\n                proj = sum(\n                    w * x[f]\n                    for w, f in zip(node.oblique_weights, node.oblique_features)\n                )\n                proj += node.oblique_bias\n                go_left = proj <= node.threshold\n            else:\n                go_left = x[node.feature] <= node.threshold\n\n            if go_left:\n                return _predict_single(node.left, x) if node.left else node.value\n            else:\n                return _predict_single(node.right, x) if node.right else node.value\n\n        preds = np.zeros((X.shape[0], self.n_outputs))\n        for i in range(X.shape[0]):\n            preds[i] = _predict_single(root, X[i])\n        return preds",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "p3mxi41a8wa",
   "source": "### Helper Functions\n\nUtilities for computing AUC, counting splits, interpretability scoring, and extracting split descriptions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vgcevtqcxhr",
   "source": "def compute_auc(y_true: np.ndarray, y_proba: np.ndarray) -> float:\n    \"\"\"Compute AUC. Handles binary and multi-class via OVR.\"\"\"\n    try:\n        n_classes = y_proba.shape[1]\n        if n_classes == 2:\n            return float(roc_auc_score(y_true, y_proba[:, 1]))\n        else:\n            return float(roc_auc_score(\n                y_true, y_proba, multi_class='ovr', average='weighted'\n            ))\n    except Exception:\n        return float('nan')\n\n\ndef count_splits_in_tree(node) -> tuple:\n    \"\"\"Count total splits and oblique splits in a tree.\"\"\"\n    if node is None or (node.left is None and node.right is None):\n        return 0, 0\n    is_obl = 1 if getattr(node, 'is_oblique', False) else 0\n    left_total, left_obl = count_splits_in_tree(node.left)\n    right_total, right_obl = count_splits_in_tree(node.right)\n    return 1 + left_total + right_total, is_obl + left_obl + right_obl\n\n\ndef count_model_splits(model) -> tuple:\n    \"\"\"Count total and oblique splits across all trees in a FIGS model.\"\"\"\n    total = 0\n    oblique = 0\n    for tree in model.trees_:\n        t, o = count_splits_in_tree(tree)\n        total += t\n        oblique += o\n    return total, oblique\n\n\ndef compute_mean_features_per_oblique(model) -> float:\n    \"\"\"Mean number of features used per oblique split.\"\"\"\n    def _collect(node, counts):\n        if node is None:\n            return\n        if getattr(node, 'is_oblique', False):\n            counts.append(len(node.oblique_features))\n        _collect(node.left, counts)\n        _collect(node.right, counts)\n\n    all_counts: list = []\n    for tree in model.trees_:\n        _collect(tree, all_counts)\n    if not all_counts:\n        return 0.0\n    return float(np.mean(all_counts))\n\n\ndef compute_interpretability_score(\n    model,\n    synergy_matrix_validate: np.ndarray,\n) -> float:\n    \"\"\"Non-circular interpretability score.\n\n    Fraction of oblique splits whose feature pairs ALL rank in top-25%\n    of synergy scores computed on the VALIDATE subset.\n    \"\"\"\n    def _collect_oblique_features(node, oblique_feats):\n        if node is None:\n            return\n        if getattr(node, 'is_oblique', False):\n            oblique_feats.append(node.oblique_features)\n        _collect_oblique_features(node.left, oblique_feats)\n        _collect_oblique_features(node.right, oblique_feats)\n\n    oblique_feats: list = []\n    for tree in model.trees_:\n        _collect_oblique_features(tree, oblique_feats)\n\n    if not oblique_feats:\n        return float('nan')\n\n    p = synergy_matrix_validate.shape[0]\n    upper_tri = synergy_matrix_validate[np.triu_indices(p, k=1)]\n    if len(upper_tri) == 0 or np.all(upper_tri == 0):\n        return 0.0\n    top25_cutoff = float(np.percentile(upper_tri, 75))\n\n    n_aligned = 0\n    for feat_list in oblique_feats:\n        all_high = True\n        for i in range(len(feat_list)):\n            for j in range(i + 1, len(feat_list)):\n                if synergy_matrix_validate[feat_list[i], feat_list[j]] < top25_cutoff:\n                    all_high = False\n                    break\n            if not all_high:\n                break\n        if all_high:\n            n_aligned += 1\n\n    return float(n_aligned / len(oblique_feats))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "n9qa3dc9tum",
   "source": "### Phase 4 — Main Experiment Loop\n\nRuns all 6 methods on each dataset with stratified K-fold CV. Computes fold-aware synergy, trains models, and collects per-fold accuracy/AUC metrics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "mcdifbnbh9e",
   "source": "def run_experiment(dataset_order, max_rules_values, n_folds, per_dataset_timeout):\n    \"\"\"Run the SG-FIGS experiment across datasets and folds.\"\"\"\n    global_start = time.time()\n    print(\"=\" * 60)\n    print(\"SG-FIGS Experiment — Starting\")\n    print(f\"  Datasets: {dataset_order}\")\n    print(f\"  Max rules: {max_rules_values}, Folds: {n_folds}\")\n    print(\"=\" * 60)\n\n    results: list = []\n    synergy_stability: dict = {}\n    method_names = [\n        'FIGS', 'RO-FIGS',\n        'SG-FIGS-10', 'SG-FIGS-25', 'SG-FIGS-50',\n        'GradientBoosting',\n    ]\n\n    for ds_idx, ds_name in enumerate(dataset_order):\n        ds_start = time.time()\n        print(f\"\\n[{ds_idx+1}/{len(dataset_order)}] Processing: {ds_name}\")\n\n        try:\n            X, y, feat_names = load_dataset(ds_name, DATASETS[ds_name])\n        except Exception as e:\n            print(f\"  Failed to load dataset {ds_name}: {e}\")\n            continue\n\n        print(f\"  Shape: {X.shape}, classes: {len(np.unique(y))}\")\n\n        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n        fold_synergy_graphs: list = []\n\n        for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n            fold_start = time.time()\n            X_train, X_test = X[train_idx], X[test_idx]\n            y_train, y_test = y[train_idx], y[test_idx]\n\n            # ── FOLD-AWARE SYNERGY COMPUTATION ──\n            n_train = len(train_idx)\n            rng = np.random.RandomState(42 + fold_idx)\n            perm = rng.permutation(n_train)\n            split_pt = int(n_train * 0.8)\n            syn_build_idx = perm[:split_pt]\n            syn_validate_idx = perm[split_pt:]\n\n            X_syn_build = X_train[syn_build_idx]\n            y_syn_build = y_train[syn_build_idx]\n            X_syn_validate = X_train[syn_validate_idx]\n            y_syn_validate = y_train[syn_validate_idx]\n\n            synergy_matrix, syn_time = compute_pairwise_synergy_matrix(\n                X_syn_build, y_syn_build,\n            )\n            fold_synergy_graphs.append(synergy_matrix)\n\n            synergy_matrix_validate = compute_fast_synergy_proxy(\n                X_syn_validate, y_syn_validate,\n            )\n\n            for max_rules in max_rules_values:\n                # ── METHOD 1: Standard FIGS ──\n                try:\n                    figs = FIGSClassifier(max_rules=max_rules, random_state=42)\n                    figs.fit(X_train, y_train)\n                    y_pred_figs = figs.predict(X_test)\n                    y_proba_figs = figs.predict_proba(X_test)\n                    acc_figs = float(accuracy_score(y_test, y_pred_figs))\n                    auc_figs = compute_auc(y_test, y_proba_figs)\n                    n_splits_figs, _ = count_model_splits(figs)\n                    results.append({\n                        'method': 'FIGS', 'dataset': ds_name, 'fold': fold_idx,\n                        'max_rules': max_rules, 'accuracy': acc_figs, 'auc': auc_figs,\n                        'n_splits': n_splits_figs, 'n_oblique': 0,\n                        'oblique_fraction': 0.0,\n                        'mean_features_per_oblique': 0.0,\n                        'interpretability_score': float('nan'),\n                        'synergy_time_s': 0.0,\n                    })\n                except Exception as e:\n                    print(f\"  FIGS failed: {e}\")\n\n                # ── METHOD 2: RO-FIGS ──\n                try:\n                    rofigs = RandomObliqueFIGS(\n                        beam_size=3, max_rules=max_rules, random_state=42,\n                    )\n                    rofigs.fit(X_train, y_train)\n                    y_pred_ro = rofigs.predict(X_test)\n                    y_proba_ro = rofigs.predict_proba(X_test)\n                    acc_ro = float(accuracy_score(y_test, y_pred_ro))\n                    auc_ro = compute_auc(y_test, y_proba_ro)\n                    n_splits_ro, n_obl_ro = count_model_splits(rofigs)\n                    results.append({\n                        'method': 'RO-FIGS', 'dataset': ds_name, 'fold': fold_idx,\n                        'max_rules': max_rules, 'accuracy': acc_ro, 'auc': auc_ro,\n                        'n_splits': n_splits_ro, 'n_oblique': n_obl_ro,\n                        'oblique_fraction': n_obl_ro / max(n_splits_ro, 1),\n                        'mean_features_per_oblique': compute_mean_features_per_oblique(rofigs),\n                        'interpretability_score': float('nan'),\n                        'synergy_time_s': 0.0,\n                    })\n                except Exception as e:\n                    print(f\"  RO-FIGS failed: {e}\")\n\n                # ── METHODS 3-5: SG-FIGS variants ──\n                for sg_name, percentile in THRESHOLDS.items():\n                    try:\n                        adj, n_edges, cutoff = build_synergy_graph(\n                            synergy_matrix, percentile,\n                        )\n                        sgfigs = SynergyGuidedFIGS(\n                            synergy_adj=adj,\n                            synergy_matrix=synergy_matrix,\n                            max_rules=max_rules,\n                            random_state=42,\n                        )\n                        sgfigs.fit(X_train, y_train)\n                        y_pred_sg = sgfigs.predict(X_test)\n                        y_proba_sg = sgfigs.predict_proba(X_test)\n                        acc_sg = float(accuracy_score(y_test, y_pred_sg))\n                        auc_sg = compute_auc(y_test, y_proba_sg)\n                        n_splits_sg, n_obl_sg = count_model_splits(sgfigs)\n                        interp_score = compute_interpretability_score(\n                            sgfigs, synergy_matrix_validate,\n                        )\n                        results.append({\n                            'method': sg_name, 'dataset': ds_name, 'fold': fold_idx,\n                            'max_rules': max_rules, 'accuracy': acc_sg, 'auc': auc_sg,\n                            'n_splits': n_splits_sg, 'n_oblique': n_obl_sg,\n                            'oblique_fraction': n_obl_sg / max(n_splits_sg, 1),\n                            'mean_features_per_oblique': compute_mean_features_per_oblique(sgfigs),\n                            'interpretability_score': interp_score,\n                            'synergy_time_s': syn_time,\n                        })\n                    except Exception as e:\n                        print(f\"  {sg_name} failed: {e}\")\n\n                # ── METHOD 6: GradientBoosting baseline ──\n                try:\n                    gbc = GradientBoostingClassifier(\n                        n_estimators=100, max_depth=3, random_state=42,\n                    )\n                    gbc.fit(X_train, y_train)\n                    y_pred_gb = gbc.predict(X_test)\n                    y_proba_gb = gbc.predict_proba(X_test)\n                    acc_gb = float(accuracy_score(y_test, y_pred_gb))\n                    auc_gb = compute_auc(y_test, y_proba_gb)\n                    results.append({\n                        'method': 'GradientBoosting', 'dataset': ds_name,\n                        'fold': fold_idx, 'max_rules': max_rules,\n                        'accuracy': acc_gb, 'auc': auc_gb,\n                        'n_splits': -1, 'n_oblique': 0,\n                        'oblique_fraction': 0.0,\n                        'mean_features_per_oblique': 0.0,\n                        'interpretability_score': float('nan'),\n                        'synergy_time_s': 0.0,\n                    })\n                except Exception as e:\n                    print(f\"  GBC failed: {e}\")\n\n            fold_elapsed = time.time() - fold_start\n            print(f\"  Fold {fold_idx} done in {fold_elapsed:.1f}s\")\n\n        # ── SYNERGY GRAPH STABILITY (Jaccard) ──\n        jaccard_pairs: list = []\n        for fi in range(len(fold_synergy_graphs)):\n            for fj in range(fi + 1, len(fold_synergy_graphs)):\n                adj_i, _, _ = build_synergy_graph(fold_synergy_graphs[fi], 75)\n                adj_j, _, _ = build_synergy_graph(fold_synergy_graphs[fj], 75)\n                edges_i = {(a, b) for a in adj_i for b in adj_i[a] if a < b}\n                edges_j = {(a, b) for a in adj_j for b in adj_j[a] if a < b}\n                union = edges_i | edges_j\n                if len(union) > 0:\n                    jacc = len(edges_i & edges_j) / len(union)\n                else:\n                    jacc = 1.0\n                jaccard_pairs.append(jacc)\n        synergy_stability[ds_name] = {\n            'mean_jaccard': float(np.mean(jaccard_pairs)) if jaccard_pairs else 0.0,\n            'std_jaccard': float(np.std(jaccard_pairs)) if jaccard_pairs else 0.0,\n        }\n\n        ds_elapsed = time.time() - ds_start\n        print(f\"  Dataset {ds_name} done in {ds_elapsed:.1f}s | Results: {len(results)}\")\n\n    total_runtime = time.time() - global_start\n    print(f\"\\n{'='*60}\")\n    print(f\"EXPERIMENT COMPLETE — {total_runtime:.1f}s, {len(results)} results\")\n    print(f\"{'='*60}\")\n\n    return results, synergy_stability, method_names",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fqbu5d48k2s",
   "source": "results, synergy_stability, method_names = run_experiment(\n    dataset_order=DATASET_ORDER,\n    max_rules_values=MAX_RULES_VALUES,\n    n_folds=N_FOLDS,\n    per_dataset_timeout=PER_DATASET_TIMEOUT,\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "krt5b8j4tq",
   "source": "### Results Visualization\n\nSummary table of mean accuracy per method, bar chart comparison across methods, and synergy graph stability (Jaccard similarity).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "66z5qtyur8",
   "source": "def visualize_results(results, synergy_stability, method_names, title_prefix=\"\"):\n    \"\"\"Reusable visualization: summary table + bar chart + synergy stability.\"\"\"\n    # ── Summary Table ──\n    mr_for_summary = max(set(r['max_rules'] for r in results))\n    print(f\"\\n{'='*60}\")\n    print(f\"{title_prefix}Summary (max_rules={mr_for_summary}, mean accuracy)\")\n    print(f\"{'='*60}\")\n    summary = {}\n    for m in method_names:\n        m_results = [r for r in results if r['method'] == m and r['max_rules'] == mr_for_summary]\n        if m_results:\n            accs = [r['accuracy'] for r in m_results]\n            aucs = [r['auc'] for r in m_results if not np.isnan(r['auc'])]\n            summary[m] = {\n                'mean_acc': np.mean(accs), 'std_acc': np.std(accs),\n                'mean_auc': np.mean(aucs) if aucs else float('nan'),\n                'n': len(m_results),\n            }\n            print(f\"  {m:20s}: {summary[m]['mean_acc']:.4f} ± {summary[m]['std_acc']:.4f}  (n={summary[m]['n']})\")\n\n    # ── Bar Chart: Mean Accuracy by Method ──\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    methods_with_data = [m for m in method_names if m in summary]\n    mean_accs = [summary[m]['mean_acc'] for m in methods_with_data]\n    std_accs = [summary[m]['std_acc'] for m in methods_with_data]\n\n    colors = ['#2196F3', '#FF9800', '#E91E63', '#9C27B0', '#673AB7', '#4CAF50']\n    bars = axes[0].bar(range(len(methods_with_data)), mean_accs,\n                       yerr=std_accs, capsize=4, color=colors[:len(methods_with_data)],\n                       edgecolor='black', linewidth=0.5)\n    axes[0].set_xticks(range(len(methods_with_data)))\n    axes[0].set_xticklabels(methods_with_data, rotation=30, ha='right', fontsize=9)\n    axes[0].set_ylabel('Mean Accuracy')\n    axes[0].set_title(f'{title_prefix}Mean Accuracy by Method (max_rules={mr_for_summary})')\n    axes[0].set_ylim(0, 1.05)\n    for bar, acc in zip(bars, mean_accs):\n        axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n                     f'{acc:.3f}', ha='center', va='bottom', fontsize=8)\n\n    # ── Per-Dataset Accuracy Heatmap ──\n    datasets_in_results = sorted(set(r['dataset'] for r in results))\n    acc_matrix = np.full((len(datasets_in_results), len(methods_with_data)), np.nan)\n    for di, ds in enumerate(datasets_in_results):\n        for mi, m in enumerate(methods_with_data):\n            ds_m = [r['accuracy'] for r in results\n                    if r['dataset'] == ds and r['method'] == m and r['max_rules'] == mr_for_summary]\n            if ds_m:\n                acc_matrix[di, mi] = np.mean(ds_m)\n\n    im = axes[1].imshow(acc_matrix, cmap='RdYlGn', aspect='auto', vmin=0.3, vmax=1.0)\n    axes[1].set_xticks(range(len(methods_with_data)))\n    axes[1].set_xticklabels(methods_with_data, rotation=30, ha='right', fontsize=9)\n    axes[1].set_yticks(range(len(datasets_in_results)))\n    axes[1].set_yticklabels(datasets_in_results, fontsize=9)\n    axes[1].set_title(f'{title_prefix}Accuracy by Dataset × Method')\n    plt.colorbar(im, ax=axes[1], label='Accuracy')\n\n    # Annotate cells\n    for di in range(len(datasets_in_results)):\n        for mi in range(len(methods_with_data)):\n            val = acc_matrix[di, mi]\n            if not np.isnan(val):\n                axes[1].text(mi, di, f'{val:.2f}', ha='center', va='center', fontsize=7,\n                            color='white' if val < 0.6 else 'black')\n\n    plt.tight_layout()\n    plt.show()\n\n    # ── Synergy Stability ──\n    if synergy_stability:\n        print(f\"\\n{'='*60}\")\n        print(f\"{title_prefix}Synergy Graph Stability (Jaccard)\")\n        print(f\"{'='*60}\")\n        for ds, stab in synergy_stability.items():\n            print(f\"  {ds:20s}: {stab['mean_jaccard']:.3f} ± {stab['std_jaccard']:.3f}\")\n\n    return summary",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "vbg6jbgkiro",
   "source": "summary_mini = visualize_results(results, synergy_stability, method_names, title_prefix=\"[Quick Demo] \")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "62hx4ivi0qa",
   "source": "---\n## Full Run — Original Parameters\n\nRuns on all 12 datasets with **5-fold CV** and `max_rules=[5, 10, 15]` — matching the original experiment parameters. This takes approximately 15-20 minutes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "i5491g0ygo",
   "source": "data = load_full()\nprint(f\"Loaded {len(data['datasets'])} datasets (full)\")\nfor ds in data['datasets']:\n    print(f\"  {ds['dataset']}: {len(ds['examples'])} examples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cft9rqgmcfi",
   "source": "# ── FULL RUN PARAMS (original) ────────────────────────────────────────────────\nDATASET_ORDER_FULL = [\n    'banknote', 'wine', 'glass',\n    'diabetes', 'heart_statlog', 'sonar',\n    'breast_cancer', 'ionosphere', 'vehicle',\n    'segment', 'credit_g', 'australian',\n]\nMAX_RULES_VALUES_FULL = [5, 10, 15]\nN_FOLDS_FULL = 5\nPER_DATASET_TIMEOUT_FULL = 600\n\nresults_full, synergy_stability_full, method_names_full = run_experiment(\n    dataset_order=DATASET_ORDER_FULL,\n    max_rules_values=MAX_RULES_VALUES_FULL,\n    n_folds=N_FOLDS_FULL,\n    per_dataset_timeout=PER_DATASET_TIMEOUT_FULL,\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "p9u9ot1ra9",
   "source": "summary_full = visualize_results(results_full, synergy_stability_full, method_names_full, title_prefix=\"[Full Run] \")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}